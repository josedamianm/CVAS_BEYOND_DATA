SUBSCRIPTION QUERY TOOL
====================================================================================================

Select query type:
  1. Query by Subscription ID (single subscription)
  2. Query by User ID (tmuserid - may return multiple subscriptions)
  3. Query by MSISDN (phone number - may return multiple subscriptions)
  0. Exit

Enter your choice (0-3): 1
Enter Subscription ID: 10151796

# ... detailed output follows ...
```

## Data Structure & Analysis

### NBS_BASE User Data Repository

The `User_Base/NBS_BASE/` directory contains daily NBS (Non-Billable Services) base data files spanning from November 2022 to present. This repository contains over 1,000 daily CSV snapshots tracking content provider services and billing information.

#### File Naming Convention

Files follow the standardized pattern: `YYYYMMDD_NBS_Base.csv`

- Example: `20221114_NBS_Base.csv` represents data from November 14, 2022
- Files are consistently named and can be sorted chronologically
- Daily files provide consistent snapshots over approximately 3 years (Nov 2022 - Present)

#### CSV Schema

Each daily NBS_Base CSV file contains the following columns:

| Column | Description |
|--------|-------------|
| `cpc` | Content Provider Code (numeric identifier) |
| `content_provider` | Provider name (e.g., "2dayuk") |
| `service_name` | Name of the service being tracked |
| `last_billed_amount` | Most recent billing amount (decimal) |
| `tme_category` | Category classification (e.g., "Light") |
| `channel_desc` | Channel description (e.g., "ACTIVE", "CCC-CR") |
| `count` | Count/quantity metric |

#### Data Analysis Commands

**Examining Data Structure:**
```bash
# View first few rows of a specific date
head -10 User_Base/NBS_BASE/20221114_NBS_Base.csv

# Count total records in a file
wc -l User_Base/NBS_BASE/20221114_NBS_Base.csv

# Check unique content providers
cut -d',' -f2 User_Base/NBS_BASE/20221114_NBS_Base.csv | sort -u

# Check date range of available data
ls -1 User_Base/NBS_BASE/ | head -1  # earliest date
ls -1 User_Base/NBS_BASE/ | tail -1  # latest date
```

**File Operations:**
```bash
# Count total files
ls -1 User_Base/NBS_BASE/ | wc -l

# Find files in a date range
ls User_Base/NBS_BASE/ | grep "202211[12]"  # November 2022

# Compare two dates (requires diff)
diff User_Base/NBS_BASE/20221114_NBS_Base.csv User_Base/NBS_BASE/20221115_NBS_Base.csv
```

#### Time Series Analysis

Data is organized chronologically, making it suitable for:
- **Daily trend analysis** - Track changes day-over-day
- **Provider activity tracking** - Monitor provider behavior over time
- **Billing amount changes** - Analyze price fluctuations
- **Service availability monitoring** - Track service status changes

#### Common Analysis Patterns

When analyzing NBS_BASE data:
1. Files can be processed sequentially by date
2. CSV format allows for easy parsing with standard tools (Python pandas, R, Excel)
3. Each file represents a complete daily snapshot
4. The `cpc` field serves as a consistent identifier for providers
5. Data continuity maintained across approximately 1,000+ daily files

#### Aggregated Outputs

The pipeline produces two aggregated views from NBS_BASE data:
- **`user_base_by_service.csv`** - User base aggregated by service
- **`user_base_by_category.csv`** - User base aggregated by category

These are generated by `01_aggregate_user_base.py` during Script 1 execution.

---

## Scheduled Job Management (Launchd)

### Overview

The CVAS pipeline runs automatically via macOS Launchd scheduled jobs:

| Script | Run Time | Launchd Job | Dependencies |
|--------|----------|-------------|--------------|
| **Script 1:** `1.GET_NBS_BASE.sh` | 8:05 AM | `com.josemanco.nbs_base` | None |
| **Script 2:** `2.FETCH_DAILY_DATA.sh` | 8:25 AM | `com.josemanco.fetch_daily` | Script 1 must complete first |
| **Script 3:** `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` | 11:30 AM | `com.josemanco.process_daily` | Script 2 must complete first |

### Change Scheduled Time

Replace `<job>` with: `nbs_base`, `fetch_daily`, or `process_daily`

1. **Edit the plist:**
   ```bash
   nvim /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

2. **Change Hour (0-23) and Minute (0-59):**
   ```xml
   <key>Hour</key>
   <integer>8</integer>     <!-- Change this -->
   <key>Minute</key>
   <integer>5</integer>     <!-- Change this -->
   ```

3. **Reload the job:**
   ```bash
   launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist && \
   launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

### Test Manually (Without Changing Schedule)

**Method 1: Run script directly**
```bash
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/1.GET_NBS_BASE.sh
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/2.FETCH_DAILY_DATA.sh
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/3.PROCESS_DAILY_AND_BUILD_VIEW.sh
```

**Method 2: Trigger via launchctl (same environment as scheduled run)**
```bash
launchctl start com.josemanco.nbs_base
launchctl start com.josemanco.fetch_daily
launchctl start com.josemanco.process_daily
```

**Check logs:**
```bash
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/1.GET_NBS_BASE.log
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/2.FETCH_DAILY_DATA.log
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/3.PROCESS_DAILY_AND_BUILD_VIEW.log
```

### Useful Commands

Replace `<job>` with: `nbs_base`, `fetch_daily`, or `process_daily`

```bash
# Check if job is loaded
launchctl list | grep com.josemanco.<job>

# View job details and next run time
launchctl print gui/$(id -u)/com.josemanco.<job>

# View last run status
launchctl print gui/$(id -u)/com.josemanco.<job> | grep -E "last exit|state"

# Unload (stop) job
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# Load (start) job
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# View real-time log
tail -f /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/<log-file>.log

# Check all jobs
launchctl list | grep josemanco

# Reload all jobs after changes
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist && \
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist
```

### Common Issues

| Issue | Solution |
|-------|----------|
| "command not found" | Use absolute paths in scripts (e.g., `/opt/anaconda3/bin/python`) |
| Job not running | Check: `launchctl list \| grep <job>` |
| Permission denied | `chmod +x <script-path>` |
| Works manually but fails in launchd | Use absolute paths, check PATH in plist |

---

## Scripts Usage Audit

### Active Pipeline Scripts

These scripts are **actively used** in the daily data pipeline orchestration:

#### Core Pipeline Scripts

| Script | Used By | Purpose |
|--------|---------|---------|
| **01_aggregate_user_base.py** | `1.GET_NBS_BASE.sh` (line 113) | Aggregates user base data from multiple CSV files |
| **02_fetch_remote_nova_data.sh** | `2.FETCH_DAILY_DATA.sh` (line 57) | Fetches transaction data from remote PostgreSQL server |
| **03_process_daily.py** | `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` (line 83) | Converts daily CSV files to Parquet format |
| **04_build_subscription_view.py** | `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` (line 105) | Builds comprehensive subscription view in DuckDB |

#### Utility Scripts

| Script | Used By | Purpose |
|--------|---------|---------|
| **utils/log_rotation.sh** | All 3 orchestration scripts | Log rotation utility (15-day retention) |

### Unused/Standalone Scripts (in Scripts/others/)

These scripts are **NOT referenced** in the main orchestration pipeline and have been organized into the `Scripts/others/` subfolder:

#### Testing/Validation Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **check_subscriptions_parquet_data.py** | Validation | Manual testing tool to verify subscription Parquet data integrity |
| **check_transactions_parquet_data.py** | Validation | Manual testing tool to verify transaction Parquet data integrity |
| **check_users.py** | Validation | Manual testing tool to verify user data |

#### Analysis/Extract Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **extract_music_subscriptions.py** | Extract/Analysis | Specialized script to extract music-specific subscription data |

#### Historical/One-Time Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **00_convert_historical.py** | Historical | One-time conversion script for historical data migration (kept in Scripts/ root) |

### Usage Summary

| Category | Count | Scripts |
|----------|-------|---------|
| **Active Pipeline** | 4 | 01_aggregate_user_base.py, 02_fetch_remote_nova_data.sh, 03_process_daily.py, 04_build_subscription_view.py |
| **Active Utils** | 1 | utils/log_rotation.sh |
| **Unused (in others/)** | 4 | check_subscriptions_parquet_data.py, check_transactions_parquet_data.py, check_users.py, extract_music_subscriptions.py |
| **Historical (kept)** | 1 | 00_convert_historical.py |
| **TOTAL** | 10 | - |

### Pipeline Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 1: 1.GET_NBS_BASE.sh                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download NBS_Base.csv from remote server (SCP)                   â”‚
â”‚ 2. Validate file                                                     â”‚
â”‚ 3. Run: 01_aggregate_user_base.py  â† ACTIVE SCRIPT                  â”‚
â”‚    â†’ Processes User_Base/NBS_BASE/*.csv files                        â”‚
â”‚    â†’ Outputs aggregated user base data                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 2: 2.FETCH_DAILY_DATA.sh                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each transaction type (ACT, RENO, DCT, PPD, CNR, RFND):         â”‚
â”‚ â†’ Run: 02_fetch_remote_nova_data.sh <type> <date>  â† ACTIVE SCRIPT  â”‚
â”‚    â†’ Connects to PostgreSQL server via SSH                           â”‚
â”‚    â†’ Fetches transaction data                                        â”‚
â”‚    â†’ Saves to Nova_Data/YYYY-MM-DD/<type>.csv                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 3: 3.PROCESS_DAILY_AND_BUILD_VIEW.sh                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Validate 6 CSV files exist                                        â”‚
â”‚ 2. Run: 03_process_daily.py <date>  â† ACTIVE SCRIPT                 â”‚
â”‚    â†’ Converts CSV files to Parquet format                            â”‚
â”‚    â†’ Saves to Parquet_Data/transactions/<type>/                      â”‚
â”‚ 3. Run: 04_build_subscription_view.py  â† ACTIVE SCRIPT               â”‚
â”‚    â†’ Loads all Parquet files                                         â”‚
â”‚    â†’ Builds comprehensive subscription view                          â”‚
â”‚    â†’ Saves to Parquet_Data/subscriptions/subscription_data.parquet   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scripts Folder Organization

```
Scripts/
â”œâ”€â”€ 00_convert_historical.py       [Historical - kept in root]
â”œâ”€â”€ 01_aggregate_user_base.py      [Active - Pipeline]
â”œâ”€â”€ 02_fetch_remote_nova_data.sh   [Active - Pipeline]
â”œâ”€â”€ 03_process_daily.py            [Active - Pipeline]
â”œâ”€â”€ 04_build_subscription_view.py  [Active - Pipeline]
â”œâ”€â”€ others/                        [Unused/standalone scripts]
â”‚   â”œâ”€â”€ check_subscriptions_parquet_data.py
â”‚   â”œâ”€â”€ check_transactions_parquet_data.py
â”‚   â”œâ”€â”€ check_users.py
â”‚   â””â”€â”€ extract_music_subscriptions.py
â””â”€â”€ utils/
    â””â”€â”€ log_rotation.sh            [Active - Utility]
```

**Key Points:**
- âœ… **Active pipeline scripts** remain in `Scripts/` root for easy access
- âœ… **Unused validation/analysis scripts** moved to `Scripts/others/`
- âœ… **Historical script** (`00_convert_historical.py`) kept in root per requirement
- âœ… **Utils folder** contains shared utilities
- âœ… **No impact on pipeline** - All orchestration scripts continue to work

**Scripts in `others/` folder:**
- Available when needed for debugging/analysis
- Not called by any orchestration scripts
- Can be used manually as needed

**Historical script:**
- `00_convert_historical.py` kept in Scripts/ root
- Available for future historical data conversions if needed

### Verification Commands

```bash
# Verify Scripts folder structure
ls -la Scripts/
ls -la Scripts/others/
ls -la Scripts/utils/

# Find all script references in orchestration scripts
grep -E "\.py|\.sh" *.sh | grep -v "^#"

# Check if any Python scripts import others
grep -r "^import\|^from" Scripts/*.py | grep "Scripts/"

# List all executable scripts
find Scripts/ -type f -perm +111
```

### Change Log

**Last Updated:** December 15, 2025
**Status:** Reorganization and renaming complete âœ…

- **2025-12-15:** Renamed scripts for better pipeline sequence clarity
  - `aggregate_user_base.py` â†’ `01_aggregate_user_base.py` (now runs before 02_fetch_remote_nova_data.sh)
  - `01_convert_historical.py` â†’ `00_convert_historical.py` (not part of regular pipeline)
- **2025-12-15:** Moved 4 unused scripts to `Scripts/others/` folder
  - check_subscriptions_parquet_data.py
  - check_transactions_parquet_data.py
  - check_users.py
  - extract_music_subscriptions.py

---

## Notes

- Scripts run sequentially: Script 1 â†’ Script 2 â†’ Script 3
- All scripts use relative paths for portability
- Data files are excluded from Git (see `.gitignore`)
- SSH key authentication required for remote data access (Scripts 1 & 2)
- Cross-platform date handling (supports both macOS and Linux)
- Historical data conversion (`00_convert_historical.py`) is interactive and prompts for data path
- Validation scripts should be run regularly to ensure data quality
- Query tool provides instant access to subscription details for troubleshooting

## License

Internal use only - Proprietary

## Author

Jose Manco
=======
# CVAS Beyond Data

> **ğŸ“– Complete Documentation:** See [`.abacus/rules.md`](.abacus/rules.md) for the comprehensive unified documentation that serves both human developers and AI assistants.

## Quick Start

**Production-grade ETL pipeline** for telecommunications subscription data processing and analytics. Automates daily collection, processing, and aggregation of CVAS transaction data.

### What It Does

- **Extract:** Downloads transaction data from remote PostgreSQL server
- **Transform:** Converts CSV to Parquet with Hive partitioning
- **Load:** Builds comprehensive subscription lifecycle views

### Daily Automated Jobs (macOS launchd)

```bash
8:05 AM  â†’ 1.GET_NBS_BASE.sh              # Download user base
8:25 AM  â†’ 2.FETCH_DAILY_DATA.sh          # Fetch 6 transaction types
11:30 AM â†’ 3.PROCESS_DAILY_AND_BUILD_VIEW.sh  # Process & aggregate
```

### Transaction Types

| Code | Name | Description |
|------|------|-------------|
| **ACT** | Activations | New subscriptions + upgrades |
| **RENO** | Renewals | Subscription renewals |
| **DCT** | Deactivations | Service cancellations |
| **CNR** | Cancellations | User-initiated cancellations |
| **RFND** | Refunds | Payment refunds |
| **PPD** | Prepaid | Prepaid transactions |

## Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Configure SSH access
ssh-copy-id omadmin@10.26.82.53

# Create directories
mkdir -p Daily_Data Parquet_Data/transactions Parquet_Data/aggregated User_Base/NBS_BASE Logs
```

## Manual Execution

```bash
# Run all scripts
bash 1.GET_NBS_BASE.sh
bash 2.FETCH_DAILY_DATA.sh
bash 3.PROCESS_DAILY_AND_BUILD_VIEW.sh

# Run specific transaction type
bash Scripts/02_fetch_remote_nova_data.sh act 2025-12-15

# Run with custom date
bash 2.FETCH_DAILY_DATA.sh 2025-12-15
bash 3.PROCESS_DAILY_AND_BUILD_VIEW.sh 2025-12-15
```

## Data Validation

```bash
# Check transaction data
python Scripts/others/check_transactions_parquet_data.py

# Check subscription data
python Scripts/others/check_subscriptions_parquet_data.py

# Interactive query tool
python Scripts/others/check_users.py
```

## Monitoring

```bash
# View logs
cat Logs/1.GET_NBS_BASE.log
cat Logs/2.FETCH_DAILY_DATA.log
cat Logs/3.PROCESS_DAILY_AND_BUILD_VIEW.log

# Real-time monitoring
tail -f Logs/3.PROCESS_DAILY_AND_BUILD_VIEW.log

# Check for errors
grep "ERROR" Logs/*.log
```

## Technology Stack

- **Python 3.x** - Core processing
- **Polars 1.34.0** - High-performance DataFrames
- **DuckDB 1.2.1** - OLAP analytics
- **PyArrow 19.0.0** - Parquet I/O
- **macOS launchd** - Automated scheduling

## Key Features

- **Incremental Processing:** Daily updates with Hive partitioning
- **Edge Case Handling:** Missing activations (~10-20%), CPC upgrades
- **Data Validation:** Strict schema enforcement via Polars
- **Comprehensive Logging:** 15-day retention with automatic rotation
- **1123+ Days History:** November 2022 to present

## Architecture

```
Daily CSV Files â†’ Parquet Storage (Hive Partitioned) â†’ Aggregated Views
```

**Data Flow:**
1. Download NBS user base + 6 transaction types (CSV)
2. Convert to Parquet with schema enforcement
3. Aggregate into comprehensive subscription view (32 columns)

## Documentation

For complete details on:
- Architecture & data flow
- Data schemas (32-column subscription view)
- Edge case handling (missing activations, CPC upgrades)
- Development guidelines
- Troubleshooting
- Launchd scheduling

**See:** [`.abacus/rules.md`](.abacus/rules.md)

## Project Structure

```
CVAS_BEYOND_DATA/
â”œâ”€â”€ 1.GET_NBS_BASE.sh              # Orchestration: NBS user base
â”œâ”€â”€ 2.FETCH_DAILY_DATA.sh          # Orchestration: Transaction fetch
â”œâ”€â”€ 3.PROCESS_DAILY_AND_BUILD_VIEW.sh  # Orchestration: Processing
â”œâ”€â”€ Scripts/                       # Processing logic
â”‚   â”œâ”€â”€ 01_aggregate_user_base.py
â”‚   â”œâ”€â”€ 02_fetch_remote_nova_data.sh
â”‚   â”œâ”€â”€ 03_process_daily.py
â”‚   â”œâ”€â”€ 04_build_subscription_view.py
â”‚   â””â”€â”€ others/                    # Validation tools
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ build_subscription_view.sql  # 241-line DuckDB query
â”œâ”€â”€ Daily_Data/                    # Temporary CSV staging
â”œâ”€â”€ Parquet_Data/                  # Columnar storage
â”‚   â”œâ”€â”€ transactions/              # Hive partitioned by type/year_month
â”‚   â””â”€â”€ aggregated/                # Final subscription view
â”œâ”€â”€ User_Base/
â”‚   â””â”€â”€ NBS_BASE/                  # 1123+ daily snapshots
â””â”€â”€ Logs/                          # Execution logs
```

## License

Internal use only - Proprietary

## Author

Jose Manco

====================================================================================================
SUBSCRIPTION QUERY TOOL
====================================================================================================

Select query type:
  1. Query by Subscription ID (single subscription)
  2. Query by User ID (tmuserid - may return multiple subscriptions)
  3. Query by MSISDN (phone number - may return multiple subscriptions)
  0. Exit

Enter your choice (0-3): 1
Enter Subscription ID: 10151796

# ... detailed output follows ...
```

## Data Structure & Analysis

### NBS_BASE User Data Repository

The `User_Base/NBS_BASE/` directory contains daily NBS (Non-Billable Services) base data files spanning from November 2022 to present. This repository contains over 1,000 daily CSV snapshots tracking content provider services and billing information.

#### File Naming Convention

Files follow the standardized pattern: `YYYYMMDD_NBS_Base.csv`

- Example: `20221114_NBS_Base.csv` represents data from November 14, 2022
- Files are consistently named and can be sorted chronologically
- Daily files provide consistent snapshots over approximately 3 years (Nov 2022 - Present)

#### CSV Schema

Each daily NBS_Base CSV file contains the following columns:

| Column | Description |
|--------|-------------|
| `cpc` | Content Provider Code (numeric identifier) |
| `content_provider` | Provider name (e.g., "2dayuk") |
| `service_name` | Name of the service being tracked |
| `last_billed_amount` | Most recent billing amount (decimal) |
| `tme_category` | Category classification (e.g., "Light") |
| `channel_desc` | Channel description (e.g., "ACTIVE", "CCC-CR") |
| `count` | Count/quantity metric |

#### Data Analysis Commands

**Examining Data Structure:**
```bash
# View first few rows of a specific date
head -10 User_Base/NBS_BASE/20221114_NBS_Base.csv

# Count total records in a file
wc -l User_Base/NBS_BASE/20221114_NBS_Base.csv

# Check unique content providers
cut -d',' -f2 User_Base/NBS_BASE/20221114_NBS_Base.csv | sort -u

# Check date range of available data
ls -1 User_Base/NBS_BASE/ | head -1  # earliest date
ls -1 User_Base/NBS_BASE/ | tail -1  # latest date
```

**File Operations:**
```bash
# Count total files
ls -1 User_Base/NBS_BASE/ | wc -l

# Find files in a date range
ls User_Base/NBS_BASE/ | grep "202211[12]"  # November 2022

# Compare two dates (requires diff)
diff User_Base/NBS_BASE/20221114_NBS_Base.csv User_Base/NBS_BASE/20221115_NBS_Base.csv
```

#### Time Series Analysis

Data is organized chronologically, making it suitable for:
- **Daily trend analysis** - Track changes day-over-day
- **Provider activity tracking** - Monitor provider behavior over time
- **Billing amount changes** - Analyze price fluctuations
- **Service availability monitoring** - Track service status changes

#### Common Analysis Patterns

When analyzing NBS_BASE data:
1. Files can be processed sequentially by date
2. CSV format allows for easy parsing with standard tools (Python pandas, R, Excel)
3. Each file represents a complete daily snapshot
4. The `cpc` field serves as a consistent identifier for providers
5. Data continuity maintained across approximately 1,000+ daily files

#### Aggregated Outputs

The pipeline produces two aggregated views from NBS_BASE data:
- **`user_base_by_service.csv`** - User base aggregated by service
- **`user_base_by_category.csv`** - User base aggregated by category

These are generated by `01_aggregate_user_base.py` during Script 1 execution.

---

## Scheduled Job Management (Launchd)

### Overview

The CVAS pipeline runs automatically via macOS Launchd scheduled jobs:

| Script | Run Time | Launchd Job | Dependencies |
|--------|----------|-------------|--------------|
| **Script 1:** `1.GET_NBS_BASE.sh` | 8:05 AM | `com.josemanco.nbs_base` | None |
| **Script 2:** `2.FETCH_DAILY_DATA.sh` | 8:25 AM | `com.josemanco.fetch_daily` | Script 1 must complete first |
| **Script 3:** `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` | 11:30 AM | `com.josemanco.process_daily` | Script 2 must complete first |

### Change Scheduled Time

Replace `<job>` with: `nbs_base`, `fetch_daily`, or `process_daily`

1. **Edit the plist:**
   ```bash
   nvim /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

2. **Change Hour (0-23) and Minute (0-59):**
   ```xml
   <key>Hour</key>
   <integer>8</integer>     <!-- Change this -->
   <key>Minute</key>
   <integer>5</integer>     <!-- Change this -->
   ```

3. **Reload the job:**
   ```bash
   launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist && \
   launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

### Test Manually (Without Changing Schedule)

**Method 1: Run script directly**
```bash
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/1.GET_NBS_BASE.sh
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/2.FETCH_DAILY_DATA.sh
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/3.PROCESS_DAILY_AND_BUILD_VIEW.sh
```

**Method 2: Trigger via launchctl (same environment as scheduled run)**
```bash
launchctl start com.josemanco.nbs_base
launchctl start com.josemanco.fetch_daily
launchctl start com.josemanco.process_daily
```

**Check logs:**
```bash
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/1.GET_NBS_BASE.log
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/2.FETCH_DAILY_DATA.log
cat /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/3.PROCESS_DAILY_AND_BUILD_VIEW.log
```

### Useful Commands

Replace `<job>` with: `nbs_base`, `fetch_daily`, or `process_daily`

```bash
# Check if job is loaded
launchctl list | grep com.josemanco.<job>

# View job details and next run time
launchctl print gui/$(id -u)/com.josemanco.<job>

# View last run status
launchctl print gui/$(id -u)/com.josemanco.<job> | grep -E "last exit|state"

# Unload (stop) job
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# Load (start) job
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# View real-time log
tail -f /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Logs/<log-file>.log

# Check all jobs
launchctl list | grep josemanco

# Reload all jobs after changes
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist && \
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist
```

### Common Issues

| Issue | Solution |
|-------|----------|
| "command not found" | Use absolute paths in scripts (e.g., `/opt/anaconda3/bin/python`) |
| Job not running | Check: `launchctl list \| grep <job>` |
| Permission denied | `chmod +x <script-path>` |
| Works manually but fails in launchd | Use absolute paths, check PATH in plist |

---

## Scripts Usage Audit

### Active Pipeline Scripts

These scripts are **actively used** in the daily data pipeline orchestration:

#### Core Pipeline Scripts

| Script | Used By | Purpose |
|--------|---------|---------|
| **01_aggregate_user_base.py** | `1.GET_NBS_BASE.sh` (line 113) | Aggregates user base data from multiple CSV files |
| **02_fetch_remote_nova_data.sh** | `2.FETCH_DAILY_DATA.sh` (line 57) | Fetches transaction data from remote PostgreSQL server |
| **03_process_daily.py** | `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` (line 83) | Converts daily CSV files to Parquet format |
| **04_build_subscription_view.py** | `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` (line 105) | Builds comprehensive subscription view in DuckDB |

#### Utility Scripts

| Script | Used By | Purpose |
|--------|---------|---------|
| **utils/log_rotation.sh** | All 3 orchestration scripts | Log rotation utility (15-day retention) |

### Unused/Standalone Scripts (in Scripts/others/)

These scripts are **NOT referenced** in the main orchestration pipeline and have been organized into the `Scripts/others/` subfolder:

#### Testing/Validation Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **check_subscriptions_parquet_data.py** | Validation | Manual testing tool to verify subscription Parquet data integrity |
| **check_transactions_parquet_data.py** | Validation | Manual testing tool to verify transaction Parquet data integrity |
| **check_users.py** | Validation | Manual testing tool to verify user data |

#### Analysis/Extract Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **extract_music_subscriptions.py** | Extract/Analysis | Specialized script to extract music-specific subscription data |

#### Historical/One-Time Scripts

| Script | Type | Likely Purpose |
|--------|------|----------------|
| **00_convert_historical.py** | Historical | One-time conversion script for historical data migration (kept in Scripts/ root) |

### Usage Summary

| Category | Count | Scripts |
|----------|-------|---------|
| **Active Pipeline** | 4 | 01_aggregate_user_base.py, 02_fetch_remote_nova_data.sh, 03_process_daily.py, 04_build_subscription_view.py |
| **Active Utils** | 1 | utils/log_rotation.sh |
| **Unused (in others/)** | 4 | check_subscriptions_parquet_data.py, check_transactions_parquet_data.py, check_users.py, extract_music_subscriptions.py |
| **Historical (kept)** | 1 | 00_convert_historical.py |
| **TOTAL** | 10 | - |

### Pipeline Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 1: 1.GET_NBS_BASE.sh                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download NBS_Base.csv from remote server (SCP)                   â”‚
â”‚ 2. Validate file                                                     â”‚
â”‚ 3. Run: 01_aggregate_user_base.py  â† ACTIVE SCRIPT                  â”‚
â”‚    â†’ Processes User_Base/NBS_BASE/*.csv files                        â”‚
â”‚    â†’ Outputs aggregated user base data                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 2: 2.FETCH_DAILY_DATA.sh                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each transaction type (ACT, RENO, DCT, PPD, CNR, RFND):         â”‚
â”‚ â†’ Run: 02_fetch_remote_nova_data.sh <type> <date>  â† ACTIVE SCRIPT  â”‚
â”‚    â†’ Connects to PostgreSQL server via SSH                           â”‚
â”‚    â†’ Fetches transaction data                                        â”‚
â”‚    â†’ Saves to Nova_Data/YYYY-MM-DD/<type>.csv                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRIPT 3: 3.PROCESS_DAILY_AND_BUILD_VIEW.sh                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Validate 6 CSV files exist                                        â”‚
â”‚ 2. Run: 03_process_daily.py <date>  â† ACTIVE SCRIPT                 â”‚
â”‚    â†’ Converts CSV files to Parquet format                            â”‚
â”‚    â†’ Saves to Parquet_Data/transactions/<type>/                      â”‚
â”‚ 3. Run: 04_build_subscription_view.py  â† ACTIVE SCRIPT               â”‚
â”‚    â†’ Loads all Parquet files                                         â”‚
â”‚    â†’ Builds comprehensive subscription view                          â”‚
â”‚    â†’ Saves to Parquet_Data/subscriptions/subscription_data.parquet   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scripts Folder Organization

```
Scripts/
â”œâ”€â”€ 00_convert_historical.py       [Historical - kept in root]
â”œâ”€â”€ 01_aggregate_user_base.py      [Active - Pipeline]
â”œâ”€â”€ 02_fetch_remote_nova_data.sh   [Active - Pipeline]
â”œâ”€â”€ 03_process_daily.py            [Active - Pipeline]
â”œâ”€â”€ 04_build_subscription_view.py  [Active - Pipeline]
â”œâ”€â”€ others/                        [Unused/standalone scripts]
â”‚   â”œâ”€â”€ check_subscriptions_parquet_data.py
â”‚   â”œâ”€â”€ check_transactions_parquet_data.py
â”‚   â”œâ”€â”€ check_users.py
â”‚   â””â”€â”€ extract_music_subscriptions.py
â””â”€â”€ utils/
    â””â”€â”€ log_rotation.sh            [Active - Utility]
```

**Key Points:**
- âœ… **Active pipeline scripts** remain in `Scripts/` root for easy access
- âœ… **Unused validation/analysis scripts** moved to `Scripts/others/`
- âœ… **Historical script** (`00_convert_historical.py`) kept in root per requirement
- âœ… **Utils folder** contains shared utilities
- âœ… **No impact on pipeline** - All orchestration scripts continue to work

**Scripts in `others/` folder:**
- Available when needed for debugging/analysis
- Not called by any orchestration scripts
- Can be used manually as needed

**Historical script:**
- `00_convert_historical.py` kept in Scripts/ root
- Available for future historical data conversions if needed

### Verification Commands

```bash
# Verify Scripts folder structure
ls -la Scripts/
ls -la Scripts/others/
ls -la Scripts/utils/

# Find all script references in orchestration scripts
grep -E "\.py|\.sh" *.sh | grep -v "^#"

# Check if any Python scripts import others
grep -r "^import\|^from" Scripts/*.py | grep "Scripts/"

# List all executable scripts
find Scripts/ -type f -perm +111
```

### Change Log

**Last Updated:** December 16, 2025
**Status:** Production fixes applied âœ…

- **2025-12-16:** Fixed row count mismatch in historical data conversion
  - Modified `00_convert_historical.py` to remove existing Parquet data before writing
  - Added `shutil.rmtree()` to prevent data appending and duplicate rows
  - Improved date parsing to handle both `YYYY-MM-DD HH:MM:SS` and `YYYY-MM-DD` formats
- **2025-12-16:** Fixed orchestration script not running
  - Corrected LaunchAgent path in `~/Library/LaunchAgents/com.josemanco.process_daily.plist`
  - Changed from incorrect `2.PROCESS_DAILY_AND_BUILD_VIEW.sh` to `3.PROCESS_DAILY_AND_BUILD_VIEW.sh`
  - Added proper environment variables and interpreter configuration
- **2025-12-16:** Fixed missing refund data in validation reports
  - Corrected date parsing for `refnd_date` column in `00_convert_historical.py`
  - Rebuilt refund Parquet data with proper date values
  - Validation now shows complete refund data by month
- **2025-12-16:** Fixed activation date validation logic
  - Modified `check_aggregated_parquet_data.py` to validate from transaction start date (2024-01-01)
  - Prevents false "missing days" errors for historical activation records
- **2025-12-15:** Renamed scripts for better pipeline sequence clarity
  - `aggregate_user_base.py` â†’ `01_aggregate_user_base.py` (now runs before 02_fetch_remote_nova_data.sh)
  - `01_convert_historical.py` â†’ `00_convert_historical.py` (not part of regular pipeline)
- **2025-12-15:** Moved 4 unused scripts to `Scripts/others/` folder
  - check_subscriptions_parquet_data.py
  - check_transactions_parquet_data.py
  - check_users.py
  - extract_music_subscriptions.py

---

## Notes

- Scripts run sequentially: Script 1 â†’ Script 2 â†’ Script 3
- All scripts use relative paths for portability
- Data files are excluded from Git (see `.gitignore`)
- SSH key authentication required for remote data access (Scripts 1 & 2)
- Cross-platform date handling (supports both macOS and Linux)
- Historical data conversion (`00_convert_historical.py`) is interactive and prompts for data path
- Validation scripts should be run regularly to ensure data quality
- Query tool provides instant access to subscription details for troubleshooting

## License

Internal use only - Proprietary

## Author

Jose Manco
