<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     ğŸš¨ğŸš¨ğŸš¨ MANDATORY INSTRUCTIONS FOR AI ASSISTANTS ğŸš¨ğŸš¨ğŸš¨
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

> ## â›” STOP - DO NOT PROCEED UNTIL YOU COMPLETE THESE STEPS
>
> **You are an AI assistant. Before answering ANY question or performing ANY task, you MUST:**
>
> ### Step 1: Read ALL Documentation Files (REQUIRED)
> Execute these reads IN ORDER. Do not skip any file:
>
> | Order | File | Purpose | Action |
> |-------|------|---------|--------|
> | 1 | `README.md` | Complete project documentation | You are reading this now âœ“ |
> | 2 | **`CONTEXT.md`** | Recent changes, session memory, project state | **READ THIS FILE NEXT** |
> | 3 | **`RULES.md`** | Architecture constraints, development rules | **READ THIS FILE AFTER CONTEXT.md** |
>
> ### Step 2: Confirm Understanding
> After reading all 3 files, you should understand:
> - [ ] This is a **telecom subscription ETL pipeline** with **4 stages**
> - [ ] Pipeline order: `1.GET_NBS_BASE.sh` â†’ `2.FETCH_DAILY_DATA.sh` â†’ `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` â†’ `4.BUILD_TRANSACTION_COUNTERS.sh`
> - [ ] Exactly **6 transaction types**: ACT, RENO, DCT, CNR, RFND, PPD
> - [ ] Tech stack: Python/Polars, DuckDB, Parquet (Hive partitioned), Shell scripts, launchd
> - [ ] Recent fixes: Refund counting (sum `rfnd_cnt`), upgrade separation, Parquet sync
> - [ ] Critical rules: Sequential execution, strict schemas, absolute Python path, no PII in logs
>
> ### âš ï¸ FAILURE TO READ ALL FILES = INCORRECT RESPONSES
> If you skip `CONTEXT.md` or `RULES.md`, you will:
> - Miss critical recent fixes and changes
> - Violate architecture constraints
> - Give incorrect advice
> - Break the pipeline
>
> ### ğŸ“‹ Quick Reference After Reading
> - **Update docs**: Edit ALL THREE files (`README.md`, `CONTEXT.md`, `RULES.md`)
> - **Add changes**: Log in `CONTEXT.md` with timestamp
> - **New constraints**: Add to `RULES.md`
> - **Python path**: Always use `/opt/anaconda3/bin/python` in shell scripts
>
> ---
>
> **ğŸ”„ NEXT ACTION: Read `CONTEXT.md` now, then read `RULES.md`**

---

## Table of Contents
- [Project Description](#project-description)
- [Architecture Overview](#architecture-overview)
- [Technology Stack](#technology-stack)
- [Directory Structure](#directory-structure)
- [Pipeline Workflow](#pipeline-workflow)
- [Installation & Setup](#installation--setup)
- [Scheduled Automation](#scheduled-automation)
- [Manual Execution](#manual-execution)
- [Monitoring & Logs](#monitoring--logs)
- [Data Schema](#data-schema)
- [Troubleshooting](#troubleshooting)

---

## Project Description

**CVAS Beyond Data** is a production-grade ETL (Extract, Transform, Load) pipeline designed for telecommunications subscription data processing and analytics. It automates the daily collection of 6 transaction types (ACT, RENO, DCT, CNR, RFND, PPD) from remote PostgreSQL servers, transforms them into optimized Parquet columnar format, and builds comprehensive lifecycle views for business analytics.

### Key Features
- **Automated Daily Execution**: Runs via macOS launchd scheduler (3 sequential jobs + 1 independent counter job).
- **Sequential Pipeline**: Strict 3-stage orchestration ensuring data consistency (1.UserBase â†’ 2.Fetch â†’ 3.Process).
- **Transaction Counters**: Independent counter system aggregating transaction metrics by CPC and Service.
- **Columnar Storage**: Parquet format with Hive partitioning (`year_month=YYYY-MM`) for efficient querying.
- **User Base Tracking**: Aggregates 1100+ daily snapshots of user base data.
- **Secure Handling**: SSH/SCP data transfer with strict PII log masking.
- **Auto-Maintenance**: 15-day log retention policy.

---

## Architecture Overview

### Four-Stage Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: USER BASE COLLECTION (8:05 AM)                        â”‚
â”‚ Script: 1.GET_NBS_BASE.sh                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download NBS_Base.csv from remote server via SCP           â”‚
â”‚ 2. Validate downloaded file integrity                         â”‚
â”‚ 3. Execute: Scripts/01_aggregate_user_base.py                 â”‚
â”‚    â†’ Process 1100+ CSV files in User_Base/NBS_BASE/           â”‚
â”‚    â†’ Generate: user_base_by_service.csv                       â”‚
â”‚    â†’ Generate: user_base_by_category.csv                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: TRANSACTION DATA FETCH (8:25 AM)                      â”‚
â”‚ Script: 2.FETCH_DAILY_DATA.sh                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each transaction type (6 types):                          â”‚
â”‚ â†’ Execute: Scripts/02_fetch_remote_nova_data.sh <type> <date> â”‚
â”‚    â†’ Connect to PostgreSQL via SSH tunnel                     â”‚
â”‚    â†’ Query transaction data for specified date                â”‚
â”‚    â†’ Save to Daily_Data/<date>/<TYPE>.csv                     â”‚
â”‚                                                                â”‚
â”‚ Transaction Types:                                            â”‚
â”‚ â€¢ ACT  - Activations (new subscriptions + upgrades)          â”‚
â”‚ â€¢ RENO - Renewals (subscription renewals)                    â”‚
â”‚ â€¢ DCT  - Deactivations (service cancellations)               â”‚
â”‚ â€¢ CNR  - Cancellations (user-initiated)                      â”‚
â”‚ â€¢ RFND - Refunds (payment refunds)                           â”‚
â”‚ â€¢ PPD  - Prepaid (prepaid transactions)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: PROCESSING & AGGREGATION (8:30 AM)                    â”‚
â”‚ Script: 3.PROCESS_DAILY_AND_BUILD_VIEW.sh                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step 1: Validate all 6 CSV files exist                        â”‚
â”‚                                                                â”‚
â”‚ Step 2: Execute Scripts/03_process_daily.py <date>            â”‚
â”‚    â†’ Convert CSV to Parquet format                            â”‚
â”‚    â†’ Apply Hive partitioning (year_month=YYYY-MM)             â”‚
â”‚    â†’ Save to Parquet_Data/transactions/<type>/                â”‚
â”‚                                                                â”‚
â”‚ Step 3: Execute Scripts/04_build_subscription_view.py         â”‚
â”‚    â†’ Load all Parquet transaction files                       â”‚
â”‚    â†’ Execute 241-line DuckDB SQL query                        â”‚
â”‚    â†’ Build comprehensive subscription lifecycle view          â”‚
â”‚    â†’ Save to Parquet_Data/aggregated/subscriptions.parquet    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: TRANSACTION COUNTERS (Independent)                    â”‚
â”‚ Script: 4.BUILD_TRANSACTION_COUNTERS.sh                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Execute: Scripts/05_build_counters.py                         â”‚
â”‚    â†’ Load transaction Parquet files                           â”‚
â”‚    â†’ Aggregate counts by CPC and Service                      â”‚
â”‚    â†’ Calculate revenue and refund metrics                     â”‚
â”‚    â†’ Split activations (free vs paid)                         â”‚
â”‚    â†’ Save to Counters/Counters_CPC.parquet                    â”‚
â”‚    â†’ Save to Counters/Counters_Service.csv                    â”‚
â”‚                                                                â”‚
â”‚ Modes:                                                        â”‚
â”‚ â€¢ Daily: Process yesterday's data (default)                  â”‚
â”‚ â€¢ Backfill: Process all missing dates                        â”‚
â”‚ â€¢ Force: Recompute existing dates                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
Remote PostgreSQL â”€â”€SSHâ”€â”€> Daily_Data (CSV) â”€â”€Pythonâ”€â”€> Parquet_Data (Columnar)
                                                                  â”‚
NBS Server â”€â”€SCPâ”€â”€> User_Base/NBS_BASE â”€â”€Pythonâ”€â”€> user_base_by_*.csv
                                                                  â”‚
                                                                  â†“
                                                    DuckDB Aggregation
                                                                  â†“
                                              subscriptions.parquet (Final View)
                                                                  â”‚
                                                                  â†“
                                            Counter Aggregation (Independent)
                                                                  â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Counters_CPC.parquet                 â”‚
                                    â”‚  Counters_Service.csv                 â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.x | Data processing and transformation |
| **Data Processing** | Polars | High-performance DataFrame operations |
| **Database** | DuckDB | In-process analytical queries |
| **Storage Format** | Parquet + Hive Partitioning | Columnar storage for efficient analytics |
| **Data Transfer** | PyArrow, Pandas | Serialization and compatibility |
| **Orchestration** | Bash Shell Scripts | Pipeline coordination |
| **Scheduling** | macOS launchd | Automated daily execution |
| **Remote Access** | SSH/SCP | Secure data retrieval |

---

## Directory Structure

```
CVAS_BEYOND_DATA/                         # Project root
â”‚
â”œâ”€â”€ 1.GET_NBS_BASE.sh                     # Stage 1 orchestrator
â”œâ”€â”€ 2.FETCH_DAILY_DATA.sh                 # Stage 2 orchestrator
â”œâ”€â”€ 3.PROCESS_DAILY_AND_BUILD_VIEW.sh     # Stage 3 orchestrator
â”œâ”€â”€ 4.BUILD_TRANSACTION_COUNTERS.sh       # Stage 4 orchestrator (independent)
â”œâ”€â”€ MASTERCPC.csv                         # Reference: Service/CPC mapping table
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ README.md                             # This file
â”‚
â”œâ”€â”€ Scripts/                              # Core processing scripts
â”‚   â”œâ”€â”€ 01_aggregate_user_base.py         # [ACTIVE] User base aggregation
â”‚   â”œâ”€â”€ 02_fetch_remote_nova_data.sh      # [ACTIVE] Remote data fetcher
â”‚   â”œâ”€â”€ 03_process_daily.py               # [ACTIVE] CSV to Parquet converter
â”‚   â”œâ”€â”€ 04_build_subscription_view.py     # [ACTIVE] Subscription aggregator
â”‚   â”œâ”€â”€ 05_build_counters.py              # [ACTIVE] Transaction counter builder
â”‚   â”œâ”€â”€ 00_convert_historical.py          # [HISTORICAL] One-time conversion
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                            # Utility scripts
â”‚   â”‚   â”œâ”€â”€ log_rotation.sh               # 15-day log retention manager
â”‚   â”‚   â””â”€â”€ counter_utils.py              # Counter system utilities
â”‚   â”‚
â”‚   â””â”€â”€ others/                           # Testing & validation scripts
â”‚       â”œâ”€â”€ check_transactions_parquet_data.py
â”‚       â”œâ”€â”€ check_subscriptions_parquet_data.py
â”‚       â”œâ”€â”€ check_users.py
â”‚       â””â”€â”€ extract_music_subscriptions.py
â”‚
â”œâ”€â”€ sql/                                  # SQL queries
â”‚   â””â”€â”€ build_subscription_view.sql       # 241-line DuckDB aggregation query
â”‚
â”œâ”€â”€ tests/                                # Unit tests
â”‚   â””â”€â”€ test_counters.py                  # Counter system tests
â”‚
â”œâ”€â”€ Daily_Data/                           # [GIT-IGNORED] Temporary CSV staging
â”‚   â””â”€â”€ YYYY-MM-DD/                       # Daily folders
â”‚       â”œâ”€â”€ ACT.csv
â”‚       â”œâ”€â”€ RENO.csv
â”‚       â”œâ”€â”€ DCT.csv
â”‚       â”œâ”€â”€ CNR.csv
â”‚       â”œâ”€â”€ RFND.csv
â”‚       â””â”€â”€ PPD.csv
â”‚
â”œâ”€â”€ Parquet_Data/                         # [GIT-IGNORED] Columnar storage
â”‚   â”œâ”€â”€ transactions/                     # Partitioned by transaction type
â”‚   â”‚   â”œâ”€â”€ act/year_month=YYYY-MM/*.parquet
â”‚   â”‚   â”œâ”€â”€ reno/year_month=YYYY-MM/*.parquet
â”‚   â”‚   â”œâ”€â”€ dct/year_month=YYYY-MM/*.parquet
â”‚   â”‚   â”œâ”€â”€ cnr/year_month=YYYY-MM/*.parquet
â”‚   â”‚   â”œâ”€â”€ ppd/year_month=YYYY-MM/*.parquet
â”‚   â”‚   â””â”€â”€ rfnd/year_month=__HIVE_DEFAULT_PARTITION__/*.parquet
â”‚   â”‚
â”‚   â””â”€â”€ aggregated/                       # Final processed data
â”‚       â””â”€â”€ subscriptions.parquet         # Comprehensive subscription view
â”‚
â”œâ”€â”€ Counters/                             # [GIT-IGNORED] Transaction counters
â”‚   â”œâ”€â”€ Counters_CPC.parquet              # CPC-level counters (historical)
â”‚   â””â”€â”€ Counters_Service.csv              # Service-level counters (historical)
â”‚
â”œâ”€â”€ User_Base/                            # User base data
â”‚   â”œâ”€â”€ NBS_BASE/                         # [GIT-IGNORED] 1100+ daily snapshots
â”‚   â”œâ”€â”€ user_base_by_service.csv          # [GIT-IGNORED] Aggregated by service
â”‚   â””â”€â”€ user_base_by_category.csv         # [GIT-IGNORED] Aggregated by category
â”‚
â””â”€â”€ Logs/                                 # [GIT-IGNORED] Execution logs
    â”œâ”€â”€ 1.GET_NBS_BASE.log
    â”œâ”€â”€ 2.FETCH_DAILY_DATA.log
    â”œâ”€â”€ 3.PROCESS_DAILY_AND_BUILD_VIEW.log
    â””â”€â”€ 4.BUILD_TRANSACTION_COUNTERS.log
```

### File Descriptions

#### Orchestration Scripts (Root Level)
| File | Purpose | Runs At | Dependencies |
|------|---------|---------|--------------|
| `1.GET_NBS_BASE.sh` | Downloads and aggregates user base | 8:05 AM | None |
| `2.FETCH_DAILY_DATA.sh` | Fetches 6 transaction types | 8:25 AM | Script 1 must complete |
| `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` | Processes and builds views | 8:30 AM | Script 2 must complete |
| `4.BUILD_TRANSACTION_COUNTERS.sh` | Builds transaction counters | 9:30 AM | Script 3 must complete (independent) |

#### Active Pipeline Scripts (Scripts/)
| Script | Called By | Purpose |
|--------|-----------|---------|
| `01_aggregate_user_base.py` | Script 1 (line 113) | Aggregates User_Base/NBS_BASE/*.csv files |
| `02_fetch_remote_nova_data.sh` | Script 2 (line 57) | Connects to PostgreSQL, fetches transactions |
| `03_process_daily.py` | Script 3 (line 83) | Converts CSV â†’ Parquet with partitioning |
| `04_build_subscription_view.py` | Script 3 (line 105) | Builds final subscription view in DuckDB |
| `05_build_counters.py` | Script 4 (line 45) | Aggregates transaction counts by CPC and Service |

#### Utility Scripts
| Script | Purpose |
|--------|---------|
| `utils/log_rotation.sh` | Deletes logs older than 15 days |
| `utils/counter_utils.py` | Counter system utilities (MASTERCPC parsing, atomic writes) |

#### Testing & Validation Scripts (Scripts/others/)
| Script | Purpose |
|--------|---------|
| `check_transactions_parquet_data.py` | Validates transaction Parquet integrity |
| `check_subscriptions_parquet_data.py` | Validates subscription Parquet integrity |
| `check_users.py` | Validates user data quality |
| `extract_music_subscriptions.py` | Extracts music-specific subscriptions |
| `query_msisdn_from_tx.py` | **NEW:** Query full history by MSISDN |
| `query_tmuserid_from_tx.py` | **NEW:** Query full history by TMUSERID |

---

## Pipeline Workflow

### Stage 1: User Base Collection (1.GET_NBS_BASE.sh)

```bash
START
  â†“
Log rotation (delete logs > 15 days)
  â†“
Download NBS_Base.csv from remote server
  â†“
Validate file exists and has content
  â†“
Execute: Scripts/01_aggregate_user_base.py
  â”œâ”€ Read all CSV files in User_Base/NBS_BASE/
  â”œâ”€ Aggregate by service type
  â”œâ”€ Aggregate by category
  â”œâ”€ Output: user_base_by_service.csv
  â””â”€ Output: user_base_by_category.csv
  â†“
Log completion timestamp
  â†“
END
```

### Stage 2: Transaction Data Fetch (2.FETCH_DAILY_DATA.sh)

```bash
START
  â†“
Log rotation (delete logs > 15 days)
  â†“
Set DATE (defaults to yesterday)
  â†“
Create directory: Daily_Data/YYYY-MM-DD/
  â†“
FOR EACH transaction type (ACT, RENO, DCT, PPD, CNR, RFND):
  â”œâ”€ Execute: Scripts/02_fetch_remote_nova_data.sh <TYPE> <DATE>
  â”‚   â”œâ”€ SSH to remote PostgreSQL server
  â”‚   â”œâ”€ Execute SQL query for transaction type
  â”‚   â”œâ”€ Save result to Daily_Data/YYYY-MM-DD/<TYPE>.csv
  â”‚   â””â”€ Log row count and status
  â””â”€ Continue to next type
  â†“
Validate all 6 CSV files exist
  â†“
Log completion timestamp
  â†“
END
```

### Stage 3: Processing & Aggregation (3.PROCESS_DAILY_AND_BUILD_VIEW.sh)

```bash
START
  â†“
Log rotation (delete logs > 15 days)
  â†“
Set DATE (defaults to yesterday)
  â†“
Validate 6 CSV files exist in Daily_Data/YYYY-MM-DD/
  â†“
Execute: Scripts/03_process_daily.py <DATE>
  â”œâ”€ Read each CSV file
  â”œâ”€ Convert to Parquet format
  â”œâ”€ Apply Hive partitioning (year_month=YYYY-MM)
  â”œâ”€ Save to Parquet_Data/transactions/<type>/
  â””â”€ Log processing statistics
  â†“
Execute: Scripts/04_build_subscription_view.py
  â”œâ”€ Load all Parquet transaction files
  â”œâ”€ Execute sql/build_subscription_view.sql (241 lines)
  â”œâ”€ Build comprehensive subscription lifecycle view
  â”‚   â”œâ”€ Join transactions with user base
  â”‚   â”œâ”€ Calculate subscription metrics
  â”‚   â”œâ”€ Aggregate revenue data
  â”‚   â””â”€ Compute lifecycle statistics
  â””â”€ Save to Parquet_Data/aggregated/subscriptions.parquet
  â†“
Log completion timestamp
  â†“
END
```

### Stage 4: Transaction Counters (4.BUILD_TRANSACTION_COUNTERS.sh)

```bash
START
  â†“
Log rotation (delete logs > 15 days)
  â†“
Set DATE (defaults to yesterday)
  â†“
Execute: Scripts/05_build_counters.py <DATE>
  â”œâ”€ Load transaction Parquet files for specified date
  â”œâ”€ Aggregate counts by CPC:
  â”‚   â”œâ”€ Count transactions by type (ACT, RENO, DCT, CNR, PPD, RFND)
  â”‚   â”œâ”€ Split activations: act_free (rev=0) vs act_pay (rev>0)
  â”‚   â”œâ”€ Sum revenue (rev) and refund amounts (rfnd_amount)
  â”‚   â””â”€ Round monetary values to 2 decimals
  â”œâ”€ Load MASTERCPC.csv mapping
  â”œâ”€ Aggregate by Service:
  â”‚   â”œâ”€ Group CPCs by service_name
  â”‚   â”œâ”€ Sum all transaction counts
  â”‚   â”œâ”€ Concatenate CPC lists per service
  â”‚   â””â”€ Calculate service-level revenue and refunds
  â”œâ”€ Merge with historical counters (idempotent updates)
  â”œâ”€ Save to Counters/Counters_CPC.parquet
  â””â”€ Save to Counters/Counters_Service.csv
  â†“
Log completion timestamp
  â†“
END
```

---

## Installation & Setup

### Prerequisites

- macOS (for launchd scheduling)
- Python 3.x
- SSH access to remote PostgreSQL server
- Sufficient disk space for data storage

### Installation Steps

1. **Clone/Navigate to Project Directory**
   ```bash
   cd /Users/josemanco/CVAS/CVAS_BEYOND_DATA
   ```

2. **Install Python Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify SSH Access**
   ```bash
   ssh user@remote-server
   ```

4. **Create Required Directories**
   ```bash
   mkdir -p Daily_Data Parquet_Data/transactions Parquet_Data/aggregated User_Base/NBS_BASE Logs
   ```

5. **Set Script Permissions**
   ```bash
   chmod +x *.sh
   chmod +x Scripts/*.sh
   chmod +x Scripts/utils/*.sh
   ```

---

## Scheduled Automation

### Launchd Configuration

The pipeline runs automatically via macOS launchd with 4 scheduled jobs:

| Job ID | Script | Schedule | Purpose |
|--------|--------|----------|---------|
| `com.josemanco.nbs_base` | `1.GET_NBS_BASE.sh` | 8:05 AM daily | User base collection |
| `com.josemanco.fetch_daily` | `2.FETCH_DAILY_DATA.sh` | 8:25 AM daily | Transaction data fetch |
| `com.josemanco.process_daily` | `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` | 8:30 AM daily | Processing & aggregation |
| `com.josemanco.build_counters` | `4.BUILD_TRANSACTION_COUNTERS.sh` | 9:30 AM daily | Transaction counters (independent) |

### Modify Schedule

Replace `<job>` with: `nbs_base`, `fetch_daily`, `process_daily`, or `build_counters`

1. **Edit the plist file:**
   ```bash
   nvim /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

2. **Change hour (0-23) and minute (0-59):**
   ```xml
   <key>Hour</key>
   <integer>8</integer>     <!-- Change this -->
   <key>Minute</key>
   <integer>5</integer>     <!-- Change this -->
   ```

3. **Reload the job:**
   ```bash
   launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist
   ```

### Useful Launchd Commands

```bash
# Check if job is loaded
launchctl list | grep com.josemanco.<job>

# View job details and next run time
launchctl print gui/$(id -u)/com.josemanco.<job>

# View last run status
launchctl print gui/$(id -u)/com.josemanco.<job> | grep -E "last exit|state"

# Manually trigger job (same environment as scheduled run)
launchctl start com.josemanco.<job>

# Unload (stop) job
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# Load (start) job
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.<job>.plist

# Check all CVAS jobs
launchctl list | grep josemanco
```

---

## Manual Execution

### Run Individual Scripts

```bash
# Stage 1: User Base Collection
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/1.GET_NBS_BASE.sh

# Stage 2: Transaction Data Fetch
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/2.FETCH_DAILY_DATA.sh

# Stage 3: Processing & Aggregation
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/3.PROCESS_DAILY_AND_BUILD_VIEW.sh
```

### Run Specific Date

```bash
# Fetch data for specific date
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/2.FETCH_DAILY_DATA.sh 2024-01-15

# Process specific date
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/3.PROCESS_DAILY_AND_BUILD_VIEW.sh 2024-01-15
```

### Run Individual Components

```bash
# Aggregate user base only
/opt/anaconda3/bin/python Scripts/01_aggregate_user_base.py

# Fetch single transaction type
bash Scripts/02_fetch_remote_nova_data.sh ACT 2024-01-15

# Process daily data
/opt/anaconda3/bin/python Scripts/03_process_daily.py 2024-01-15

# Build subscription view
/opt/anaconda3/bin/python Scripts/04_build_subscription_view.py
```

### Run Transaction Counters

The counter system runs independently from the main pipeline and aggregates transaction counts by CPC and Service.

```bash
# Daily mode (default): Process yesterday's data
bash 4.BUILD_TRANSACTION_COUNTERS.sh

# Process specific date
bash 4.BUILD_TRANSACTION_COUNTERS.sh 2024-01-15

# Backfill mode: Process all missing dates from transaction data
bash 4.BUILD_TRANSACTION_COUNTERS.sh --backfill

# Force recompute existing dates
bash 4.BUILD_TRANSACTION_COUNTERS.sh 2024-01-15 --force

# Initial historical load (backfill + force)
bash 4.BUILD_TRANSACTION_COUNTERS.sh --backfill --force

# Direct Python execution
/opt/anaconda3/bin/python Scripts/05_build_counters.py 2024-01-15
/opt/anaconda3/bin/python Scripts/05_build_counters.py --backfill
```

**Output Files:**
- `Counters/Counters_CPC.parquet` - CPC-level counters (historical, 15 columns)
- `Counters/Counters_Service.csv` - Service-level counters (historical, 21 columns)

**Counter Columns:**
- Transaction counts: `act_count` (non-upgrade), `act_free`, `act_pay`, `upg_count` (upgrades), `reno_count`, `dct_count`, `upg_dct_count` (upgrade deactivations), `cnr_count`, `ppd_count`, `rfnd_count`
- Financial metrics: `rfnd_amount`, `rev` (total revenue)
- Service metadata (Service CSV only): `Free_CPC`, `Free_Period`, `Upgrade_CPC`, `CHG_Period`, `CHG_Price`
- Metadata: `date`, `cpc`/`service_name`, `tme_category`, `cpcs`, `last_updated`

**Execution Modes:**
- **Daily mode**: Processes yesterday's data (default)
- **Backfill mode**: Auto-discovers and processes all missing dates
- **Force mode**: Recomputes existing dates (idempotent updates)

### Data Querying Tools

Validate data for specific users using the provided utility scripts:

```bash
# Query by MSISDN (automatically adds country code '34' if missing)
# Outputs: Act/Reno history, revenue summary, and refunds
python Scripts/others/query_msisdn_from_tx.py 34686516147

# Query by TMUSERID
# Outputs: Linked MSISDNs and full transaction history
python Scripts/others/query_tmuserid_from_tx.py 8343817051345500000
```

### Query Transaction Data

Query subscription lifecycle by MSISDN or TMUSERID:

```bash
# Query by MSISDN (automatically adds country code '34' if missing)
python Scripts/others/query_msisdn_from_tx.py 686516147
python Scripts/others/query_msisdn_from_tx.py 34686516147

# Query by TMUSERID
python Scripts/others/query_tmuserid_from_tx.py 8343817051345500000
```

**Output includes:**
- MSISDN â†” TMUSERID mapping (all unique identifiers associated)
- Full subscription lifecycle grouped by `subscription_id`:
  - ACT (Activations)
  - RENO (Renewals)
  - DCT (Deactivations)
  - CNR (Cancellations)
  - RFND (Refunds)
- Summary statistics (counts per transaction type, total revenue, total refunded)
- PPD (Pay Per Download) one-time purchases (displayed separately)

**Query Logic:**
1. Step 1: Find all `subscription_id`s associated with the identifier (from ACT, RENO, DCT)
2. Step 2: Retrieve all transactions (ACT, RENO, DCT, CNR, RFND) for those subscription_ids
3. Step 3: Query PPD transactions directly by the original identifier

---

## Monitoring & Logs

### Log Files Location

```
Logs/
â”œâ”€â”€ 1.GET_NBS_BASE.log                    # Stage 1 execution log
â”œâ”€â”€ 2.FETCH_DAILY_DATA.log                # Stage 2 execution log
â”œâ”€â”€ 3.PROCESS_DAILY_AND_BUILD_VIEW.log    # Stage 3 execution log
â””â”€â”€ 4.BUILD_TRANSACTION_COUNTERS.log      # Stage 4 execution log
```

### View Logs

```bash
# View full log
cat Logs/1.GET_NBS_BASE.log

# View last 50 lines
tail -n 50 Logs/2.FETCH_DAILY_DATA.log

# Real-time monitoring
tail -f Logs/3.PROCESS_DAILY_AND_BUILD_VIEW.log

# Monitor counter system
tail -f Logs/4.BUILD_TRANSACTION_COUNTERS.log

# Search for errors
grep -i error Logs/*.log

# Check today's execution
grep "$(date +%Y-%m-%d)" Logs/*.log
```

### Log Rotation

- **Retention Period:** 15 days
- **Managed By:** `Scripts/utils/log_rotation.sh`
- **Executed:** At the start of each orchestration script
- **Command:** `find Logs/ -name "*.log" -mtime +15 -delete`

---

## Data Schema

### Transaction Types

| Code | Name | Description | Has Revenue | Volume |
|------|------|-------------|-------------|--------|
| **ACT** | Activations | New subscriptions + upgrades | âœ… Yes | High |
| **RENO** | Renewals | Subscription renewals | âœ… Yes | Highest |
| **DCT** | Deactivations | Service cancellations | âŒ No | Medium |
| **CNR** | Cancellations | User-initiated cancellations | âŒ No | Low |
| **RFND** | Refunds | Payment refunds | âœ… Yes (negative) | Low |
| **PPD** | Prepaid | Prepaid transactions | âœ… Yes | Medium |

### Key Data Files

| File | Format | Size | Purpose |
|------|--------|------|---------|
| `MASTERCPC.csv` | CSV | ~500 KB | Service/CPC mapping reference |
| `user_base_by_service.csv` | CSV | ~2 MB | User base aggregated by service |
| `user_base_by_category.csv` | CSV | ~1 MB | User base aggregated by category |
| `subscriptions.parquet` | Parquet | Varies | Final comprehensive subscription view |
| `Counters_CPC.parquet` | Parquet | ~1 MB | CPC-level transaction counters (historical) |
| `Counters_Service.csv` | CSV | ~4.5 MB | Service-level transaction counters (historical) |

### Counter Schemas

#### Counters_CPC.parquet (15 columns)
| Column | Type | Description |
|--------|------|-------------|
| `date` | Date | Transaction date |
| `cpc` | Int64 | Content Provider Code |
| `act_count` | Int64 | Non-upgrade activations (channel_act != 'UPGRADE') |
| `act_free` | Int64 | Free non-upgrade activations (rev=0, channel_act != 'UPGRADE') |
| `act_pay` | Int64 | Paid non-upgrade activations (rev>0, channel_act != 'UPGRADE') |
| `upg_count` | Int64 | Upgrade activations (channel_act == 'UPGRADE') |
| `reno_count` | Int64 | Renewal count |
| `dct_count` | Int64 | Deactivation count |
| `upg_dct_count` | Int64 | Upgrade deactivations (channel_dct == 'UPGRADE') |
| `cnr_count` | Int64 | Cancellation count |
| `ppd_count` | Int64 | Prepaid transaction count |
| `rfnd_count` | Int64 | Refund count |
| `rfnd_amount` | Float64 | Total refund amount (2 decimals) |
| `rev` | Float64 | Total revenue (2 decimals) |
| `last_updated` | Datetime | Last update timestamp |

#### Counters_Service.csv (21 columns)
| Column | Type | Description |
|--------|------|-------------|
| `date` | Date | Transaction date |
| `service_name` | String | Service name from MASTERCPC |
| `tme_category` | String | TME category |
| `cpcs` | String | Comma-separated list of CPCs |
| `Free_CPC` | Int64 | Free CPC from MASTERCPC |
| `Free_Period` | Int64 | Free period from MASTERCPC |
| `Upgrade_CPC` | Int64 | Upgrade CPC from MASTERCPC |
| `CHG_Period` | Int64 | Charge period from MASTERCPC |
| `CHG_Price` | Float64 | Charge price from MASTERCPC |
| `act_count` | Int64 | Non-upgrade activations (channel_act != 'UPGRADE') |
| `act_free` | Int64 | Free non-upgrade activations (rev=0, channel_act != 'UPGRADE') |
| `act_pay` | Int64 | Paid non-upgrade activations (rev>0, channel_act != 'UPGRADE') |
| `upg_count` | Int64 | Upgrade activations (channel_act == 'UPGRADE') |
| `reno_count` | Int64 | Renewal count |
| `dct_count` | Int64 | Deactivation count |
| `upg_dct_count` | Int64 | Upgrade deactivations (channel_dct == 'UPGRADE') |
| `cnr_count` | Int64 | Cancellation count |
| `ppd_count` | Int64 | Prepaid transaction count |
| `rfnd_count` | Int64 | Refund count |
| `rfnd_amount` | Float64 | Total refund amount (2 decimals) |
| `rev` | Float64 | Total revenue (2 decimals) |

**Counter Features:**
- **Idempotent updates**: Can reprocess dates without duplicates
- **Nubico filtering**: Excludes services containing "nubico" (case-insensitive)
- **Auto-discovery**: Backfill mode finds all missing dates automatically
- **Force mode**: Recompute existing dates with `--force` flag
- **Upgrade separation**: Upgrades are tracked separately from regular activations
  - `act_count` excludes upgrades (channel_act != 'UPGRADE')
  - `upg_count` contains only upgrades (channel_act == 'UPGRADE')
  - Total activations = `act_count` + `upg_count`

### Parquet Partitioning Strategy

- **Method:** Hive-style partitioning
- **Partition Key:** `year_month=YYYY-MM`
- **Structure:** `Parquet_Data/transactions/<type>/year_month=YYYY-MM/*.parquet`
- **Benefits:**
  - Faster query performance (partition pruning)
  - Organized data by time period
  - Efficient storage management
  - Easy data lifecycle management

---

## Troubleshooting

### Common Issues

| Issue | Possible Cause | Solution |
|-------|---------------|----------|
| **"command not found"** | launchd PATH issues | Use absolute paths (e.g., `/opt/anaconda3/bin/python`) |
| **Job not running** | Job not loaded in launchd | `launchctl list \| grep <job>` |
| **Permission denied** | Script not executable | `chmod +x <script-path>` |
| **SSH connection failed** | SSH keys not configured | Set up SSH key authentication |
| **Missing CSV files** | Stage 2 incomplete | Check `2.FETCH_DAILY_DATA.log` for errors |
| **Parquet write failed** | Disk space insufficient | Check disk space: `df -h` |
| **Works manually but fails in launchd** | Environment differences | Add absolute paths in scripts |

### Validation Commands

```bash
# Check pipeline status
launchctl list | grep josemanco

# Verify data exists
ls -lh Daily_Data/$(date +%Y-%m-%d)/
ls -lh Parquet_Data/transactions/act/
ls -lh Counters/

# Count records in Parquet
python -c "import polars as pl; print(pl.read_parquet('Parquet_Data/aggregated/subscriptions.parquet').shape)"

# Check counter files
python -c "import polars as pl; print(f'CPC Counters: {pl.read_parquet(\"Counters/Counters_CPC.parquet\").shape}')"
wc -l Counters/Counters_Service.csv

# Verify all 6 transaction types
for type in ACT RENO DCT PPD CNR RFND; do
    echo -n "$type: "
    wc -l "Daily_Data/$(date -v-1d +%Y-%m-%d)/$type.csv"
done

# Check disk usage
du -sh Parquet_Data/
du -sh User_Base/NBS_BASE/
du -sh Counters/

# Query specific user data (for debugging)
python Scripts/others/query_msisdn_from_tx.py <msisdn>
python Scripts/others/query_tmuserid_from_tx.py <tmuserid>
```

### Emergency Recovery

```bash
# Stop all jobs
launchctl unload /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist

# Clear problematic data
rm -rf Daily_Data/$(date +%Y-%m-%d)/

# Rerun specific stage
bash /Users/josemanco/CVAS/CVAS_BEYOND_DATA/2.FETCH_DAILY_DATA.sh $(date +%Y-%m-%d)

# Restart all jobs
launchctl load /Users/josemanco/Library/LaunchAgents/com.josemanco.*.plist
```

### Debug Mode

Enable verbose logging by editing scripts:

```bash
# At the top of any .sh script, add:
set -x  # Enable debug mode
set -e  # Exit on error
```

---

## Project Maintenance

### Regular Tasks

| Task | Frequency | Command |
|------|-----------|---------|
| **Check logs** | Daily | `tail -100 Logs/*.log` |
| **Monitor disk space** | Weekly | `du -sh Parquet_Data/ User_Base/` |
| **Verify data quality** | Weekly | Run validation scripts in `Scripts/others/` |
| **Review job status** | Daily | `launchctl list \| grep josemanco` |
| **Archive old data** | Monthly | Move old Parquet partitions to archive |

### Data Retention Policy

- **Daily CSV Files:** Deleted after Parquet conversion (manual cleanup)
- **Logs:** 15-day retention (automatic)
- **Parquet Data:** Indefinite (manual archive when needed)
- **User Base Snapshots:** All snapshots retained (1100+ files)

---

## Developer Guidelines

This project follows these conventions:

1. **Sequential Execution Required:** Scripts 1 â†’ 2 â†’ 3 must run in order
2. **All 6 Transaction Types Required:** Pipeline fails if any CSV is missing
3. **Absolute Paths in Automation:** launchd requires full paths (e.g., `/opt/anaconda3/bin/python`)
4. **Hive Partitioning:** All Parquet files use `year_month=YYYY-MM` partitioning
5. **No Manual Directory Changes:** Structure is fixed, scripts use relative paths from project root
6. **15-Day Log Retention:** Logs auto-delete after 15 days
7. **User Base Aggregation:** Processes 1100+ daily snapshots into 2 summary CSV files
8. **DuckDB for Analytics:** Final subscription view built with 241-line SQL query
9. **Columnar Storage:** All analytics data stored as Parquet for performance
10. **macOS Specific:** Uses launchd (macOS scheduling system), not cron

### Key Directories
- **Source of Truth:** `Parquet_Data/aggregated/subscriptions.parquet`
- **Reference Data:** `MASTERCPC.csv` (service/CPC mapping)
- **Active Scripts:** `Scripts/01_*.py`, `Scripts/02_*.sh`, `Scripts/03_*.py`, `Scripts/04_*.py`, `Scripts/05_*.py`
- **Counter Data:** `Counters/Counters_CPC.parquet`, `Counters/Counters_Service.csv`
- **Temporary Data:** `Daily_Data/` (CSV files, can be deleted after processing)

---

**Project Maintained By:** Jose Manco  
**Project Path:** `/Users/josemanco/CVAS/CVAS_BEYOND_DATA`  
**Last Updated:** 2025-01-27
