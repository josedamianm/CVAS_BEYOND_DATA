# CVAS Beyond Data - Telecommunications ETL Pipeline

> **Last Updated**: 2026-02-15
>
> **AI Agents**: Read `CLAUDE.md` for complete context, rules, and session history

---

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [ETL Data Flow](#etl-data-flow)
- [Architecture](#architecture)
- [Technology Stack](#technology-stack)
- [Directory Structure](#directory-structure)
- [Pipeline Stages](#pipeline-stages)
- [Installation & Setup](#installation--setup)
- [Usage](#usage)
- [Monitoring](#monitoring)
- [Data Schemas](#data-schemas)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

---

## ğŸ¯ Project Overview

**CVAS Beyond Data** is a production-grade ETL pipeline for telecommunications subscription analytics. It processes millions of transaction records daily, transforming raw PostgreSQL data into optimized Parquet format for business intelligence and reporting.

### Key Capabilities
- **Automated Daily Processing**: Scheduled execution via macOS launchd (8:05 AM - 9:30 AM)
- **6 Transaction Types**: ACT (activations), RENO (renewals), DCT (deactivations), CNR (cancellations), RFND (refunds), PPD (prepaid)
- **High-Performance Storage**: Hive-partitioned Parquet with SNAPPY compression
- **Subscription Lifecycle Tracking**: DuckDB-powered aggregation for complete user journeys
- **Business Metrics**: Daily CPC-level and service-level transaction counters

### Performance Metrics
- **Daily Processing Time**: ~1.5 hours (full pipeline)
- **Historical Data**: 1,123+ user base snapshots
- **Transaction Volume**: Millions of records/month
- **Data Format**: Parquet (columnar, compressed)

---

## ğŸ”„ ETL Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CVAS BEYOND DATA ETL PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 0: Configuration Setup (Manual/As-Needed)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  0.GET_MASTERCPC_CSV.py                                                      â”‚
â”‚  â”œâ”€ Reads: Master CPCs Excel files                                          â”‚
â”‚  â”œâ”€ Processes: CPC metadata (service names, categories, prices, periods)    â”‚
â”‚  â”œâ”€ Special Logic: Sets cpc_period=99999 for PPD transactions               â”‚
â”‚  â””â”€ Outputs: MASTERCPC.csv (CPCâ†’Service mapping)                            â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: Extract User Base (8:05 AM) - Duration: ~5 min                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  1.GET_NBS_BASE.sh â†’ Scripts/01_aggregate_user_base.py                      â”‚
â”‚  â”œâ”€ Extracts: NBS_Base.csv from Nova PostgreSQL (via SCP)                   â”‚
â”‚  â”œâ”€ Transforms: Aggregates by service, category, and CPC                    â”‚
â”‚  â”‚   â€¢ Excludes: nubico, challenge arena, movistar apple music, juegos onmo â”‚
â”‚  â”‚   â€¢ Maps: education/images â†’ Edu_Ima, news/sports â†’ News_Sport          â”‚
â”‚  â””â”€ Loads:                                                                   â”‚
â”‚      â€¢ User_Base/YYYYMMDD_NBS_Base.csv (raw snapshot)                       â”‚
â”‚      â€¢ User_Base/user_base_by_service.csv (service-level aggregation)       â”‚
â”‚      â€¢ User_Base/user_base_by_category.csv (category-level aggregation)     â”‚
â”‚      â€¢ User_Base/user_base_by_cpc.csv (CPC-level aggregation)               â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: Extract Transactions (8:25 AM) - Duration: ~10 min                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  2.FETCH_DAILY_DATA.sh â†’ Scripts/02_fetch_remote_nova_data.sh               â”‚
â”‚  â”œâ”€ Extracts: Yesterday's transactions from Nova PostgreSQL                 â”‚
â”‚  â”‚   For each type (ACT, RENO, DCT, CNR, RFND, PPD):                        â”‚
â”‚  â”‚   â€¢ Connects via SSH to remote server (10.26.82.53)                      â”‚
â”‚  â”‚   â€¢ Executes SQL query to create temp table                              â”‚
â”‚  â”‚   â€¢ Exports to CSV on remote server                                      â”‚
â”‚  â”‚   â€¢ Downloads via SCP to local Daily_Data/                               â”‚
â”‚  â””â”€ Loads:                                                                   â”‚
â”‚      â€¢ Daily_Data/act_atlas_day.csv                                          â”‚
â”‚      â€¢ Daily_Data/reno_atlas_day.csv                                         â”‚
â”‚      â€¢ Daily_Data/dct_atlas_day.csv                                          â”‚
â”‚      â€¢ Daily_Data/cnr_atlas_day.csv                                          â”‚
â”‚      â€¢ Daily_Data/rfnd_atlas_day.csv                                         â”‚
â”‚      â€¢ Daily_Data/ppd_atlas_day.csv                                          â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: Transform & Load (8:30 AM) - Duration: ~45 min                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  3.PROCESS_DAILY_AND_BUILD_VIEW.sh                                          â”‚
â”‚  â”œâ”€ Step 3A: Scripts/03_process_daily.py                                    â”‚
â”‚  â”‚   â”œâ”€ Reads: Daily CSV files from Stage 2                                 â”‚
â”‚  â”‚   â”œâ”€ Transforms:                                                          â”‚
â”‚  â”‚   â”‚   â€¢ Applies strict Polars schemas (type enforcement)                 â”‚
â”‚  â”‚   â”‚   â€¢ Parses date columns to Datetime                                  â”‚
â”‚  â”‚   â”‚   â€¢ Adds year_month partition column (YYYY-MM)                       â”‚
â”‚  â”‚   â”‚   â€¢ Deduplicates:                                                     â”‚
â”‚  â”‚   â”‚     - ACT/RENO/DCT/PPD: by (subscription_id, trans_date, trans_type_id) â”‚
â”‚  â”‚   â”‚     - CNR: by (sbn_id, cancel_date)                                  â”‚
â”‚  â”‚   â”‚     - RFND: by (sbnid, refnd_date)                                   â”‚
â”‚  â”‚   â””â”€ Loads: Parquet_Data/transactions/{type}/year_month=YYYY-MM/*.parquet â”‚
â”‚  â”‚                                                                            â”‚
â”‚  â””â”€ Step 3B: Scripts/04_build_subscription_view.py                          â”‚
â”‚      â”œâ”€ Reads: All transaction Parquet files                                â”‚
â”‚      â”œâ”€ Transforms (DuckDB SQL):                                             â”‚
â”‚      â”‚   â€¢ Unions ACT + RENO transactions                                   â”‚
â”‚      â”‚   â€¢ Tracks CPC changes/upgrades per subscription                     â”‚
â”‚      â”‚   â€¢ Identifies first transaction (activation proxy)                  â”‚
â”‚      â”‚   â€¢ Aggregates renewals, deactivations, cancellations, refunds       â”‚
â”‚      â”‚   â€¢ Calculates subscription status and lifetime                      â”‚
â”‚      â”‚   â€¢ Excludes upgrade deactivations (channel_dct != 'UPGRADE')        â”‚
â”‚      â””â”€ Loads: Parquet_Data/aggregated/subscriptions.parquet                â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: Build Counters (9:30 AM) - Duration: ~15 min [INDEPENDENT]         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  4.BUILD_TRANSACTION_COUNTERS.sh â†’ Scripts/05_build_counters.py             â”‚
â”‚  â”œâ”€ Reads:                                                                   â”‚
â”‚  â”‚   â€¢ Parquet_Data/transactions/{type}/**/*.parquet (all transaction types)â”‚
â”‚  â”‚   â€¢ MASTERCPC.csv (CPC metadata)                                         â”‚
â”‚  â”œâ”€ Transforms:                                                              â”‚
â”‚  â”‚   â€¢ Computes daily counts by CPC:                                        â”‚
â”‚  â”‚     - act_count (excludes channel_act='UPGRADE')                         â”‚
â”‚  â”‚     - act_free (rev=0, non-upgrade)                                      â”‚
â”‚  â”‚     - act_pay (rev>0, non-upgrade)                                       â”‚
â”‚  â”‚     - upg_count (channel_act='UPGRADE')                                  â”‚
â”‚  â”‚     - reno_count                                                          â”‚
â”‚  â”‚     - dct_count (excludes channel_dct='UPGRADE')                         â”‚
â”‚  â”‚     - upg_dct_count (channel_dct='UPGRADE')                              â”‚
â”‚  â”‚     - cnr_count                                                           â”‚
â”‚  â”‚     - ppd_count                                                           â”‚
â”‚  â”‚     - rfnd_count (SUM of rfnd_cnt column, NOT row count!)                â”‚
â”‚  â”‚     - rfnd_amount, rev                                                    â”‚
â”‚  â”‚   â€¢ Merges with historical counters (idempotent)                         â”‚
â”‚  â”‚   â€¢ Joins with MASTERCPC.csv for service metadata                        â”‚
â”‚  â”‚   â€¢ Filters out: nubico, challenge arena, movistar apple music, juegos onmo â”‚
â”‚  â””â”€ Loads:                                                                   â”‚
â”‚      â€¢ Counters/Counters_CPC.parquet (historical CPC-level counters)        â”‚
â”‚      â€¢ Counters/Counters_Service.csv (CPC-level with service metadata)      â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MAINTENANCE: Historical Data Regeneration (As-Needed)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  Scripts/00_convert_historical.py                                           â”‚
â”‚  â”œâ”€ Purpose: One-time or refresh conversion of historical CSV to Parquet    â”‚
â”‚  â”œâ”€ Reads: Historical_Data/{act,reno,dct,cnr,rfnd,ppd}_atlas.csv           â”‚
â”‚  â”œâ”€ Transforms: Same as Stage 3A (schemas, partitioning, deduplication)     â”‚
â”‚  â””â”€ Loads: Parquet_Data/transactions/{type}/year_month=*/*.parquet          â”‚
â”‚                                                                               â”‚
â”‚  When to run:                                                                â”‚
â”‚  â€¢ After historical CSV files are updated                                   â”‚
â”‚  â€¢ When Parquet data becomes stale                                          â”‚
â”‚  â€¢ Before backfilling counters                                              â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Architecture

### Pipeline Dependencies

```
Stage 0 (Manual) â”€â”€â”
                   â”œâ”€â”€> Stage 1 â”€â”€> Stage 2 â”€â”€> Stage 3 â”€â”€> Stage 4
                   â”‚                                           â†‘
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       (MASTERCPC.csv required for Stage 4)

CRITICAL: Stages 1â†’2â†’3 MUST run sequentially
Stage 4 is independent but requires Stage 3 completion
```

### Data Storage Architecture

```
Parquet_Data/
â”œâ”€â”€ transactions/              # Hive-partitioned transaction data
â”‚   â”œâ”€â”€ act/
â”‚   â”‚   â”œâ”€â”€ year_month=2024-01/
â”‚   â”‚   â”œâ”€â”€ year_month=2024-02/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ reno/year_month=*/
â”‚   â”œâ”€â”€ dct/year_month=*/
â”‚   â”œâ”€â”€ cnr/year_month=*/
â”‚   â”œâ”€â”€ rfnd/year_month=*/
â”‚   â””â”€â”€ ppd/year_month=*/
â””â”€â”€ aggregated/
    â””â”€â”€ subscriptions.parquet  # Subscription lifecycle view
```

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Data Processing** | Python 3.x + Polars | High-performance DataFrame operations |
| **SQL Aggregation** | DuckDB | In-process OLAP for subscription views |
| **Storage Format** | Parquet (SNAPPY) | Columnar compression, Hive partitioning |
| **Data Source** | PostgreSQL | Remote Nova server (10.26.82.53) |
| **Orchestration** | Bash Scripts | Pipeline stage coordination |
| **Scheduler** | macOS launchd | Automated daily execution |
| **Data Transfer** | SSH/SCP | Secure remote data fetching |

---

## ğŸ“ Directory Structure

```
CVAS_BEYOND_DATA/
â”œâ”€â”€ README.md                            # This file (human-readable overview)
â”œâ”€â”€ CLAUDE.md                            # AI agent context (rules, history, schemas)
â”œâ”€â”€ MASTERCPC.csv                        # CPCâ†’Service mapping (generated by Stage 0)
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”‚
â”œâ”€â”€ 0.GET_MASTERCPC_CSV.py               # Stage 0: Generate CPC metadata
â”œâ”€â”€ 1.GET_NBS_BASE.sh                    # Stage 1: Fetch user base
â”œâ”€â”€ 2.FETCH_DAILY_DATA.sh                # Stage 2: Fetch transactions
â”œâ”€â”€ 3.PROCESS_DAILY_AND_BUILD_VIEW.sh    # Stage 3: Transform & load
â”œâ”€â”€ 4.BUILD_TRANSACTION_COUNTERS.sh      # Stage 4: Build counters
â”‚
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ 00_convert_historical.py         # Maintenance: Historical CSVâ†’Parquet
â”‚   â”œâ”€â”€ 01_aggregate_user_base.py        # Stage 1: User base aggregation
â”‚   â”œâ”€â”€ 02_fetch_remote_nova_data.sh     # Stage 2: Remote data fetching
â”‚   â”œâ”€â”€ 03_process_daily.py              # Stage 3A: Daily CSVâ†’Parquet
â”‚   â”œâ”€â”€ 04_build_subscription_view.py    # Stage 3B: Subscription lifecycle
â”‚   â”œâ”€â”€ 05_build_counters.py             # Stage 4: Counter generation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ counter_utils.py             # Counter helper functions
â”‚       â””â”€â”€ log_rotation.sh              # Log management (15-day retention)
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ build_subscription_view.sql      # DuckDB aggregation query
â”‚
â”œâ”€â”€ Daily_Data/                          # Daily CSV files (gitignored)
â”‚   â”œâ”€â”€ act_atlas_day.csv
â”‚   â”œâ”€â”€ reno_atlas_day.csv
â”‚   â”œâ”€â”€ dct_atlas_day.csv
â”‚   â”œâ”€â”€ cnr_atlas_day.csv
â”‚   â”œâ”€â”€ rfnd_atlas_day.csv
â”‚   â””â”€â”€ ppd_atlas_day.csv
â”‚
â”œâ”€â”€ Parquet_Data/                        # Parquet storage (gitignored)
â”‚   â”œâ”€â”€ transactions/
â”‚   â”‚   â”œâ”€â”€ act/year_month=*/
â”‚   â”‚   â”œâ”€â”€ reno/year_month=*/
â”‚   â”‚   â”œâ”€â”€ dct/year_month=*/
â”‚   â”‚   â”œâ”€â”€ cnr/year_month=*/
â”‚   â”‚   â”œâ”€â”€ rfnd/year_month=*/
â”‚   â”‚   â””â”€â”€ ppd/year_month=*/
â”‚   â””â”€â”€ aggregated/
â”‚       â””â”€â”€ subscriptions.parquet
â”‚
â”œâ”€â”€ User_Base/                           # User base snapshots (gitignored)
â”‚   â”œâ”€â”€ NBS_BASE/
â”‚   â”‚   â””â”€â”€ YYYYMMDD_NBS_Base.csv
â”‚   â”œâ”€â”€ user_base_by_service.csv
â”‚   â”œâ”€â”€ user_base_by_category.csv
â”‚   â””â”€â”€ user_base_by_cpc.csv
â”‚
â”œâ”€â”€ Counters/                            # Counter outputs (gitignored)
â”‚   â”œâ”€â”€ Counters_CPC.parquet             # Historical CPC-level counters
â”‚   â””â”€â”€ Counters_Service.csv             # CPC-level with service metadata
â”‚
â””â”€â”€ Logs/                                # Pipeline logs (gitignored)
    â”œâ”€â”€ 1_get_nbs_base_YYYYMMDD.log
    â”œâ”€â”€ 2_fetch_daily_data_YYYYMMDD.log
    â”œâ”€â”€ 3_process_daily_YYYYMMDD.log
    â””â”€â”€ 4_build_counters_YYYYMMDD.log
```

---

## ğŸš€ Pipeline Stages

### Stage 0: Generate CPC Metadata (Manual)
**Script**: `0.GET_MASTERCPC_CSV.py`
**Trigger**: Manual execution when CPC master files are updated
**Purpose**: Generate `MASTERCPC.csv` from Excel master files
**Output**: `MASTERCPC.csv` (cpc, service_name, tme_category, cpc_period, cpc_price)
**Special Logic**: Sets `cpc_period=99999` for PPD transactions

### Stage 1: Extract User Base (8:05 AM)
**Script**: `1.GET_NBS_BASE.sh` â†’ `Scripts/01_aggregate_user_base.py`
**Duration**: ~5 minutes
**Purpose**: Fetch and aggregate daily user base snapshot
**Outputs**:
- `User_Base/NBS_BASE/YYYYMMDD_NBS_Base.csv` (raw snapshot)
- `User_Base/user_base_by_service.csv` (service-level aggregation)
- `User_Base/user_base_by_category.csv` (category-level aggregation)
- `User_Base/user_base_by_cpc.csv` (CPC-level aggregation)

### Stage 2: Extract Transactions (8:25 AM)
**Script**: `2.FETCH_DAILY_DATA.sh` â†’ `Scripts/02_fetch_remote_nova_data.sh`  
**Duration**: ~10 minutes  
**Purpose**: Fetch yesterday's transactions for all 6 types  
**Outputs**: `Daily_Data/{act,reno,dct,cnr,rfnd,ppd}_atlas_day.csv`

### Stage 3: Transform & Load (8:30 AM)
**Script**: `3.PROCESS_DAILY_AND_BUILD_VIEW.sh`  
**Duration**: ~45 minutes  
**Sub-stages**:
- **3A**: `Scripts/03_process_daily.py` - Convert CSVs to Parquet with deduplication
- **3B**: `Scripts/04_build_subscription_view.py` - Build subscription lifecycle view

**Outputs**:
- `Parquet_Data/transactions/{type}/year_month=YYYY-MM/*.parquet`
- `Parquet_Data/aggregated/subscriptions.parquet`

### Stage 4: Build Counters (9:30 AM) [INDEPENDENT]
**Script**: `4.BUILD_TRANSACTION_COUNTERS.sh` â†’ `Scripts/05_build_counters.py`  
**Duration**: ~15 minutes  
**Purpose**: Generate daily transaction counters by CPC  
**Outputs**:
- `Counters/Counters_CPC.parquet` (historical CPC-level counters)
- `Counters/Counters_Service.csv` (CPC-level with service metadata)

**Modes**:
- Daily: Process yesterday's date
- Backfill: Process date range
- Force: Overwrite existing data

---

## âš™ï¸ Installation & Setup

### Prerequisites
```bash
# Python 3.x with required packages
pip install -r requirements.txt

# Or install manually:
pip install polars duckdb pandas python-dateutil pyarrow
```

### Configuration Steps

1. **Update PostgreSQL Connection**
   Edit `Scripts/02_fetch_remote_nova_data.sh`:
   ```bash
   REMOTE_USER="omadmin"
   REMOTE_HOST="10.26.82.53"
   REMOTE_DB="nova"
   ```

2. **Verify Python Path**
   All scripts use: `/opt/anaconda3/bin/python`  
   Update if your Python installation differs.

3. **Generate MASTERCPC.csv**
   ```bash
   /opt/anaconda3/bin/python 0.GET_MASTERCPC_CSV.py
   ```

4. **Set Up launchd (Optional - for automation)**
   ```bash
   # Copy plist files to LaunchAgents
   cp launchd/*.plist ~/Library/LaunchAgents/
   
   # Load jobs
   launchctl load ~/Library/LaunchAgents/com.cvas.stage1.plist
   launchctl load ~/Library/LaunchAgents/com.cvas.stage2.plist
   launchctl load ~/Library/LaunchAgents/com.cvas.stage3.plist
   launchctl load ~/Library/LaunchAgents/com.cvas.stage4.plist
   ```

---

## ğŸ’» Usage

### Manual Execution

#### Run Full Pipeline
```bash
cd /Users/josemanco/CVAS/CVAS_BEYOND_DATA

# Sequential execution (Stages 1-4)
./1.GET_NBS_BASE.sh && \
./2.FETCH_DAILY_DATA.sh && \
./3.PROCESS_DAILY_AND_BUILD_VIEW.sh && \
./4.BUILD_TRANSACTION_COUNTERS.sh
```

#### Run Individual Stages
```bash
# Stage 1: User Base
./1.GET_NBS_BASE.sh

# Stage 2: Fetch Transactions
./2.FETCH_DAILY_DATA.sh

# Stage 3: Transform & Load
./3.PROCESS_DAILY_AND_BUILD_VIEW.sh

# Stage 4: Build Counters (with options)
./4.BUILD_TRANSACTION_COUNTERS.sh                    # Daily mode
./4.BUILD_TRANSACTION_COUNTERS.sh --force            # Force overwrite
./4.BUILD_TRANSACTION_COUNTERS.sh --backfill         # Backfill all dates
./4.BUILD_TRANSACTION_COUNTERS.sh 2025-01-15         # Specific date
./4.BUILD_TRANSACTION_COUNTERS.sh --start-date 2025-01-01 --end-date 2025-01-31
```

### Maintenance Tasks

#### Regenerate Historical Parquet Data
```bash
# When historical CSV files are updated
/opt/anaconda3/bin/python Scripts/00_convert_historical.py

# Then rebuild counters
./4.BUILD_TRANSACTION_COUNTERS.sh --backfill --force
```

#### Generate CPC Metadata
```bash
# When master CPC Excel files are updated
/opt/anaconda3/bin/python 0.GET_MASTERCPC_CSV.py
```

---

## ğŸ“Š Monitoring

### Log Files
```
Logs/
â”œâ”€â”€ 1_get_nbs_base_YYYYMMDD.log
â”œâ”€â”€ 2_fetch_daily_data_YYYYMMDD.log
â”œâ”€â”€ 3_process_daily_YYYYMMDD.log
â””â”€â”€ 4_build_counters_YYYYMMDD.log
```

**Log Retention**: 15 days (automatic rotation via `Scripts/utils/log_rotation.sh`)

### Check Pipeline Status
```bash
# View latest logs
tail -f Logs/4_build_counters_$(date +%Y%m%d).log

# Check launchd job status
launchctl list | grep com.cvas

# Verify output files
ls -lh Counters/
ls -lh Parquet_Data/transactions/act/year_month=2025-02/
```

---

## ğŸ“‹ Data Schemas

### Transaction Types

| Type | Columns | Key Fields | Notes |
|------|---------|------------|-------|
| **ACT** | 15 | subscription_id, cpc, trans_date, channel_act, rev | Activations (excludes UPGRADE) |
| **RENO** | 15 | subscription_id, cpc, trans_date, channel_act, rev | Renewals |
| **DCT** | 13 | subscription_id, cpc, trans_date, channel_dct | Deactivations (excludes UPGRADE) |
| **CNR** | 5 | sbn_id, cpc, cancel_date, mode | Cancellations |
| **RFND** | 7 | sbnid, cpc, refnd_date, rfnd_cnt, rfnd_amount | Refunds (sum rfnd_cnt!) |
| **PPD** | 15 | subscription_id, cpc, trans_date, rev | Prepaid (cpc_period=99999) |

### Output Files

#### Counters_CPC.parquet (13 columns)
```
date, cpc, act_count, act_free, act_pay, upg_count, reno_count, 
dct_count, upg_dct_count, cnr_count, ppd_count, rfnd_count, 
rfnd_amount, rev, last_updated
```

#### Counters_Service.csv (18 columns)
```
date, service_name, tme_category, cpc, cpc_period, cpc_price, 
act_count, act_free, act_pay, upg_count, reno_count, dct_count, 
upg_dct_count, cnr_count, ppd_count, rfnd_count, rfnd_amount, rev
```

#### MASTERCPC.csv (5 columns)
```
cpc, service_name, tme_category, cpc_period, cpc_price
```

---

## ğŸ”§ Troubleshooting

### Issue: Pipeline Stage Fails

**Symptoms**: Stage exits with error code

**Checklist**:
1. Check logs in `Logs/` directory
2. Verify previous stage completed successfully
3. Ensure data files exist in expected locations
4. Verify Python path: `/opt/anaconda3/bin/python`
5. Check disk space and permissions

### Issue: Counter Mismatches

**Symptoms**: Counters don't match manual CSV counts

**Common Causes**:
1. **Refund undercounting**: Ensure `rfnd_cnt` column is summed, not row count
2. **Stale Parquet data**: Run `Scripts/00_convert_historical.py`
3. **Upgrade inclusion**: Verify `channel_act != 'UPGRADE'` and `channel_dct != 'UPGRADE'` filters

**Solution**:
```bash
# Regenerate Parquet files
/opt/anaconda3/bin/python Scripts/00_convert_historical.py

# Rebuild counters
./4.BUILD_TRANSACTION_COUNTERS.sh --backfill --force
```

### Issue: Missing MASTERCPC.csv

**Symptoms**: Stage 4 fails with "MASTERCPC.csv not found"

**Solution**:
```bash
/opt/anaconda3/bin/python 0.GET_MASTERCPC_CSV.py
```

### Issue: SSH/SCP Connection Failures

**Symptoms**: Stage 2 cannot connect to remote server

**Checklist**:
1. Verify SSH keys are configured
2. Test manual connection: `ssh omadmin@10.26.82.53`
3. Check network connectivity
4. Verify remote server is accessible

---

## ğŸ¤ Contributing

This project is maintained with AI assistance (Abacus AI Desktop, Claude Sonnet 4.5).

**For AI Agents**: Read `CLAUDE.md` for complete context, critical rules, session history, and development guidelines.

**For Humans**: Follow existing code patterns, maintain schema consistency, and update documentation when making changes.

---

## ğŸ“„ License

Jose Manco Only

---

**Project Root**: `/Users/josemanco/CVAS/CVAS_BEYOND_DATA`  
**Python Environment**: `/opt/anaconda3/bin/python`  
**Remote Server**: `omadmin@10.26.82.53` (Nova PostgreSQL)
