# CVAS Beyond Data - AI Development Rules & Context

> **Purpose:** This document defines critical rules, constraints, and context for AI assistants when modifying code in the CVAS Beyond Data pipeline. Read the `README.md` for general project documentation.

---

## üö® CRITICAL ARCHITECTURE CONSTRAINTS

### 1. Sequential Pipeline Dependency (NEVER BREAK)

**RULE:** Scripts MUST execute in strict order. Each depends on the previous completing successfully.

```
1.GET_NBS_BASE.sh (8:05 AM) ‚Üí 2.FETCH_DAILY_DATA.sh (8:25 AM) ‚Üí 3.PROCESS_DAILY_AND_BUILD_VIEW.sh (11:30 AM)
```

**Why:** Script 2 needs yesterday's data. Script 3 needs all 6 transaction CSV files from Script 2.

**DO NOT:**
- ‚ùå Make scripts independent
- ‚ùå Add parallel execution
- ‚ùå Remove dependency validation
- ‚ùå Change execution order

**DO:**
- ‚úÖ Validate previous stage completed before starting
- ‚úÖ Log dependencies clearly
- ‚úÖ Exit with error if prerequisites missing

---

### 2. Six Transaction Types (NEVER CHANGE COUNT)

**RULE:** Exactly 6 transaction types must be processed. Adding/removing types breaks the entire pipeline.

```
ACT, RENO, DCT, CNR, RFND, PPD
```

**Why:** DuckDB aggregation query expects all 6. Missing types cause SQL failures.

**DO NOT:**
- ‚ùå Remove any transaction type
- ‚ùå Add new types without updating ALL components
- ‚ùå Make any type optional

**DO:**
- ‚úÖ If adding type: Update `02_fetch_remote_nova_data.sh`, `03_process_daily.py`, `04_build_subscription_view.py`, `sql/build_subscription_view.sql`
- ‚úÖ Validate all 6 CSV files exist before processing
- ‚úÖ Use consistent case (uppercase) everywhere

---

### 3. Directory Structure (IMMUTABLE)

**RULE:** Never modify the directory structure. Scripts use relative paths from project root.

```
CVAS_BEYOND_DATA/
‚îú‚îÄ‚îÄ 1.GET_NBS_BASE.sh
‚îú‚îÄ‚îÄ 2.FETCH_DAILY_DATA.sh  
‚îú‚îÄ‚îÄ 3.PROCESS_DAILY_AND_BUILD_VIEW.sh
‚îú‚îÄ‚îÄ Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_aggregate_user_base.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_fetch_remote_nova_data.sh
‚îÇ   ‚îú‚îÄ‚îÄ 03_process_daily.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_build_subscription_view.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/log_rotation.sh
‚îÇ   ‚îî‚îÄ‚îÄ others/ (validation scripts)
‚îú‚îÄ‚îÄ sql/build_subscription_view.sql
‚îú‚îÄ‚îÄ Daily_Data/ (gitignored)
‚îú‚îÄ‚îÄ Parquet_Data/ (gitignored)
‚îú‚îÄ‚îÄ User_Base/ (gitignored)
‚îî‚îÄ‚îÄ Logs/ (gitignored)
```

**DO NOT:**
- ‚ùå Move orchestration scripts (1, 2, 3) out of root
- ‚ùå Rename the `Scripts/` directory
- ‚ùå Change Parquet storage structure
- ‚ùå Reorganize folder hierarchy

**DO:**
- ‚úÖ Use relative paths: `Scripts/01_aggregate_user_base.py`
- ‚úÖ Keep new validation scripts in `Scripts/others/`
- ‚úÖ Maintain Hive partitioning structure in Parquet_Data

---

### 4. Strict Schema Enforcement (NON-NEGOTIABLE)

**RULE:** All Parquet files must follow exact schemas. Schema violations cause aggregation failures.

**Key Schemas:**

#### ACT/RENO/PPD (15 columns - with revenue):
```python
{
    'tmuserid': pl.Utf8,
    'msisdn': pl.Utf8,
    'cpc': pl.Int64,
    'trans_type_id': pl.Int64,
    'channel_id': pl.Int64,
    'channel_act': pl.Utf8,
    'trans_date': pl.Datetime,
    'act_date': pl.Datetime,
    'reno_date': pl.Datetime,
    'camp_name': pl.Utf8,
    'tef_prov': pl.Int64,
    'campana_medium': pl.Utf8,
    'campana_id': pl.Utf8,
    'subscription_id': pl.Int64,  # PRIMARY KEY
    'rev': pl.Float64
}
```

#### DCT (13 columns - no revenue):
```python
# Same as above minus 'rev', plus:
{'channel_dct': pl.Utf8}
```

#### CNR (5 columns):
```python
{
    'cancel_date': pl.Datetime,
    'sbn_id': pl.Int64,  # subscription_id
    'tmuserid': pl.Utf8,
    'cpc': pl.Int64,
    'mode': pl.Utf8
}
```

#### RFND (7 columns):
```python
{
    'tmuserid': pl.Utf8,
    'cpc': pl.Int64,
    'refnd_date': pl.Datetime,
    'rfnd_amount': pl.Float64,
    'rfnd_cnt': pl.Int64,
    'sbnid': pl.Int64,  # subscription_id
    'instant_rfnd': pl.Utf8
}
```

**DO NOT:**
- ‚ùå Change column names (breaks SQL queries)
- ‚ùå Modify data types
- ‚ùå Add optional columns
- ‚ùå Remove existing columns

**DO:**
- ‚úÖ Enforce schemas in `03_process_daily.py` using Polars
- ‚úÖ Validate CSV columns before Parquet conversion
- ‚úÖ Fail loudly if schema mismatch detected

---

### 5. Hive Partitioning (REQUIRED FOR PERFORMANCE)

**RULE:** All transaction Parquet files MUST use Hive partitioning by `year_month=YYYY-MM`.

```
Parquet_Data/transactions/act/year_month=2025-01/*.parquet
Parquet_Data/transactions/act/year_month=2025-02/*.parquet
```

**Why:** DuckDB uses partition pruning for 100x faster queries.

**Implementation:**
```python
df = df.with_columns(pl.lit(year_month).alias('year_month'))
df.write_parquet(
    path,
    use_pyarrow=True,
    pyarrow_options={'partition_cols': ['year_month']}
)
```

**DO NOT:**
- ‚ùå Remove partition column
- ‚ùå Use different partitioning scheme
- ‚ùå Flatten Parquet structure
- ‚ùå Change date format (must be YYYY-MM)

**DO:**
- ‚úÖ Always add `year_month` column before writing
- ‚úÖ Use format: `YYYY-MM` (e.g., "2025-01")
- ‚úÖ Maintain folder structure: `<type>/year_month=<value>/`

---

### 6. Absolute Python Path in Shell Scripts (LAUNCHD REQUIREMENT)

**RULE:** Shell scripts MUST use absolute path to Python for launchd compatibility.

```bash
# CORRECT (launchd-compatible):
/opt/anaconda3/bin/python Scripts/03_process_daily.py

# INCORRECT (fails in launchd):
python Scripts/03_process_daily.py
python3 Scripts/03_process_daily.py
```

**Why:** launchd runs with minimal PATH. Relative commands fail.

**DO NOT:**
- ‚ùå Use `python` or `python3` commands
- ‚ùå Rely on PATH environment variable
- ‚ùå Use virtualenv activation

**DO:**
- ‚úÖ Use full path: `/opt/anaconda3/bin/python`
- ‚úÖ Test scripts with: `launchctl start com.josemanco.<job>`
- ‚úÖ Use absolute paths for all external commands in plist files

---

## üõ†Ô∏è DEVELOPMENT RULES

### Path Management

**RULE:** Use relative paths from project root, except for Python interpreter.

```bash
# CORRECT:
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"
/opt/anaconda3/bin/python Scripts/01_aggregate_user_base.py

# INCORRECT:
/opt/anaconda3/bin/python /Users/josemanco/CVAS/CVAS_BEYOND_DATA/Scripts/01_aggregate_user_base.py
```

**DO NOT:**
- ‚ùå Hardcode absolute paths (except Python)
- ‚ùå Assume current working directory
- ‚ùå Use `~` expansion in scripts

**DO:**
- ‚úÖ Calculate paths relative to `$SCRIPT_DIR`
- ‚úÖ Use `cd` to project root before executing
- ‚úÖ Verify paths exist before using

---

### Cross-Platform Date Handling

**RULE:** Support both macOS and Linux date commands.

```bash
# CORRECT (cross-platform):
if [[ "$OSTYPE" == "darwin"* ]]; then
    yday=$(date -v-1d +%Y-%m-%d)  # macOS
else
    yday=$(date -d "yesterday" +%Y-%m-%d)  # Linux
fi

# INCORRECT (macOS-only):
yday=$(date -v-1d +%Y-%m-%d)
```

**DO NOT:**
- ‚ùå Use macOS-only date syntax
- ‚ùå Assume Linux date format
- ‚ùå Skip OS detection

**DO:**
- ‚úÖ Check `$OSTYPE` before date operations
- ‚úÖ Test on both platforms if modifying date logic
- ‚úÖ Use ISO format: `YYYY-MM-DD`

---

### Log Rotation (MANDATORY)

**RULE:** All orchestration scripts MUST call log rotation at start.

```bash
# At the top of every orchestration script:
source "$(dirname "$0")/Scripts/utils/log_rotation.sh"
rotate_log "$LOGFILE"
```

**Why:** Prevents disk space issues. Logs accumulate quickly (15+ days = several GB).

**DO NOT:**
- ‚ùå Remove log rotation calls
- ‚ùå Change retention period without approval
- ‚ùå Skip rotation for new scripts

**DO:**
- ‚úÖ Source `log_rotation.sh` in all new scripts
- ‚úÖ Call `rotate_log "$LOGFILE"` before logging
- ‚úÖ Keep 15-day retention (default)

---

### Error Handling Pattern

**RULE:** All scripts must exit with non-zero code on failure and log errors.

```bash
# CORRECT:
if [ ! -f "$FILE" ]; then
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ‚úó ERROR: File not found: $FILE" >> "$LOGFILE"
    exit 1
fi

# INCORRECT:
if [ ! -f "$FILE" ]; then
    echo "File not found"
fi
```

**DO NOT:**
- ‚ùå Continue execution after errors
- ‚ùå Exit with code 0 on failure
- ‚ùå Suppress error messages

**DO:**
- ‚úÖ Use `set -e` at top of bash scripts
- ‚úÖ Log errors with timestamp: `[YYYY-MM-DD HH:MM:SS] ‚úó ERROR: ...`
- ‚úÖ Validate inputs before processing
- ‚úÖ Exit with code 1 on any failure

---

### SQL Query Management

**RULE:** Complex SQL queries (200+ lines) belong in `sql/` directory, NOT embedded in Python.

```python
# CORRECT:
sql_file = project_root / 'sql' / 'build_subscription_view.sql'
query = sql_file.read_text()
query = query.replace('{parquet_path}', str(parquet_path))

# INCORRECT:
query = """
SELECT ... (200 lines)
"""
```

**DO NOT:**
- ‚ùå Embed long SQL in Python strings
- ‚ùå Hard-code paths in SQL files
- ‚ùå Split complex queries across multiple files

**DO:**
- ‚úÖ Store in `sql/` directory
- ‚úÖ Use template variables: `{parquet_path}`
- ‚úÖ Replace variables before execution
- ‚úÖ Keep SQL readable with proper formatting

---

### Python Dependencies (PINNED VERSIONS)

**RULE:** Never change library versions without testing. Polars/PyArrow compatibility is fragile.

```
polars==1.34.0
pyarrow==19.0.0
duckdb==1.2.1
pandas==2.2.3
```

**DO NOT:**
- ‚ùå Upgrade Polars without testing
- ‚ùå Use `>=` or `~` version specifiers
- ‚ùå Add new dependencies without justification

**DO:**
- ‚úÖ Pin exact versions in `requirements.txt`
- ‚úÖ Test thoroughly after version changes
- ‚úÖ Document why each library is needed

---

## üîí DATA GOVERNANCE RULES

### User Base Category Mapping (BUSINESS LOGIC)

**RULE:** Category mappings are business-defined. Never change without approval.

```python
# Education + Images ‚Üí "Edu_Ima"
# News + Sports ‚Üí "News_Sport"

# EXCLUSIONS (case-insensitive):
# - Services containing "nubico"
# - Services containing "challenge arena"
```

**Location:** `Scripts/01_aggregate_user_base.py:27-53`

**DO NOT:**
- ‚ùå Modify category names
- ‚ùå Change exclusion rules
- ‚ùå Remove business logic

**DO:**
- ‚úÖ Get approval before changing mappings
- ‚úÖ Document reasons for exclusions
- ‚úÖ Test with real data after changes

---

### NBS_BASE Immutability

**RULE:** Historical NBS_BASE CSV files (1123+ files) are immutable snapshots. Never modify.

**Naming:** `YYYYMMDD_NBS_Base.csv` (e.g., `20220818_NBS_Base.csv`)

**DO NOT:**
- ‚ùå Edit historical CSV files
- ‚ùå Delete old snapshots
- ‚ùå Regenerate past dates

**DO:**
- ‚úÖ Only add new daily files
- ‚úÖ Preserve original formatting
- ‚úÖ Validate new files match schema

---

### PII Protection (SECURITY)

**RULE:** Never log or export PII to unsecured locations.

**PII Fields:**
- `tmuserid` (user ID)
- `msisdn` (phone number)

**DO NOT:**
- ‚ùå Log `tmuserid` or `msisdn` values in production logs
- ‚ùå Print PII in error messages
- ‚ùå Export PII to unencrypted files

**DO:**
- ‚úÖ Log counts/aggregates only
- ‚úÖ Use `subscription_id` for debugging
- ‚úÖ Mask PII in logs: `tmuserid: ***1234`

**EXCEPTION:** Query/debugging scripts (`Scripts/others/query_*.py`, `check_users.py`) are allowed to display PII when explicitly queried by the user for troubleshooting purposes. These scripts:
- Are NOT part of the automated pipeline
- Require manual execution with explicit user input
- Display results to terminal only (not logged to files)
- Are used for operational debugging and support

**Examples of Allowed PII Display:**
- `query_msisdn_from_tx.py` - Shows MSISDN and associated TMUSERIDs when queried
- `query_tmuserid_from_tx.py` - Shows TMUSERID and associated MSISDNs when queried
- `check_users.py` - Displays user details when queried by subscription_id

**Examples of Prohibited PII Logging:**
- Pipeline scripts (`01_aggregate_user_base.py`, `03_process_daily.py`, etc.) must NEVER log PII
- Orchestration scripts (`1.GET_NBS_BASE.sh`, etc.) must NEVER log PII
- Error messages in automated processes must use `subscription_id` only

---

### Git Ignore Enforcement

**RULE:** Never commit data files. Only commit code, configs, and documentation.

**NEVER COMMIT:**
- ‚ùå `Daily_Data/` (temporary CSV files)
- ‚ùå `Parquet_Data/` (large binary files)
- ‚ùå `User_Base/NBS_BASE/` (1123+ snapshot files)
- ‚ùå `User_Base/*.csv` (aggregated outputs)
- ‚ùå `Logs/` (execution logs)

**DO:**
- ‚úÖ Commit `.sh` scripts
- ‚úÖ Commit `.py` scripts
- ‚úÖ Commit `.sql` files
- ‚úÖ Commit `requirements.txt`
- ‚úÖ Commit `MASTERCPC.csv` (reference data)

---

## üêõ EDGE CASES & COMMON PITFALLS

### Edge Case 1: Missing Activation Records

**Problem:** Some subscriptions start with RENO (renewal) without prior ACT (activation).

**Solution:** Treat first transaction (ACT or RENO) as activation. Flag with `missing_act_record = true`.

**Location:** `sql/build_subscription_view.sql:69-95`

**DO NOT:**
- ‚ùå Skip subscriptions without ACT
- ‚ùå Assume activation always exists

**DO:**
- ‚úÖ Use COALESCE to handle missing ACT
- ‚úÖ Set flag for tracking
- ‚úÖ Use first_renewal_date as fallback

---

### Edge Case 2: CPC Upgrades

**Problem:** Subscriptions can change services (CPC codes) mid-lifecycle.

**Solution:** Track all CPCs chronologically in `cpc_list`. Detect upgrades via `trans_type_id = 1` in ACT table.

**Location:** `sql/build_subscription_view.sql:50-108`

**DO NOT:**
- ‚ùå Overwrite CPC on upgrade
- ‚ùå Ignore previous services
- ‚ùå Assume one CPC per subscription

**DO:**
- ‚úÖ Maintain ordered list: `cpc_list`
- ‚úÖ Track: `first_cpc`, `current_cpc`, `upgraded_to_cpc`
- ‚úÖ Set `has_upgraded = true` flag

---

### Edge Case 3: Subscription Status Hierarchy

**Problem:** A subscription can have both deactivation and cancellation.

**Solution:** Follow strict precedence: DCT > CNR > ACTIVE

```sql
subscription_status = CASE
    WHEN deactivation_date IS NOT NULL THEN 'DEACTIVATED'
    WHEN cancellation_date IS NOT NULL THEN 'CANCELLED'
    ELSE 'ACTIVE'
END
```

**DO NOT:**
- ‚ùå Use OR logic for status
- ‚ùå Change precedence order

**DO:**
- ‚úÖ Deactivation overrides cancellation
- ‚úÖ Check DCT first, then CNR
- ‚úÖ Default to ACTIVE if neither

---

### Pitfall 1: RFND Partitioning

**Issue:** Refund dates can be NULL, causing Hive partitioning to fail.

**Solution:** Use `__HIVE_DEFAULT_PARTITION__` for NULL year_month values.

```
Parquet_Data/transactions/rfnd/year_month=__HIVE_DEFAULT_PARTITION__/*.parquet
```

**DO NOT:**
- ‚ùå Skip NULL refund dates
- ‚ùå Use "null" or "unknown" as partition value

**DO:**
- ‚úÖ Use exact string: `__HIVE_DEFAULT_PARTITION__`
- ‚úÖ Handle in DuckDB queries with NULL check

---

### Pitfall 2: Launchd Environment Differences

**Issue:** Scripts work manually but fail in launchd.

**Root Cause:** launchd runs with minimal environment (no PATH, HOME, etc.)

**Solution:**
1. Use absolute path to Python: `/opt/anaconda3/bin/python`
2. Use absolute paths in plist `ProgramArguments`
3. Set `WorkingDirectory` in plist

**DO NOT:**
- ‚ùå Assume PATH is set
- ‚ùå Use commands without full path
- ‚ùå Rely on shell aliases

**DO:**
- ‚úÖ Test with: `launchctl start com.josemanco.<job>`
- ‚úÖ Check logs: `Logs/<script>.log`
- ‚úÖ Use absolute paths everywhere in plist

---

### Pitfall 3: Parquet Compression

**Issue:** Wrong compression reduces performance.

**Solution:** Always use SNAPPY (balance of speed and size).

```python
# CORRECT:
df.write_parquet(path, compression='snappy')

# INCORRECT (too slow):
df.write_parquet(path, compression='gzip')
```

**DO NOT:**
- ‚ùå Use GZIP (10x slower to decompress)
- ‚ùå Use UNCOMPRESSED (wastes disk space)

**DO:**
- ‚úÖ Always use SNAPPY
- ‚úÖ Specify explicitly in code

---

## ‚ö° PERFORMANCE RULES

### DuckDB Query Optimization

**RULE:** Always filter on `year_month` partition column for time-based queries.

```sql
-- GOOD (uses partition pruning):
SELECT * FROM 'Parquet_Data/transactions/act/**/*.parquet'
WHERE year_month = '2025-01'

-- BAD (scans all partitions):
SELECT * FROM 'Parquet_Data/transactions/act/**/*.parquet'
WHERE trans_date >= '2025-01-01'
```

**DO NOT:**
- ‚ùå Filter on `trans_date` without `year_month`
- ‚ùå Skip partition filters

**DO:**
- ‚úÖ Filter on `year_month` first
- ‚úÖ Add `trans_date` filter as secondary
- ‚úÖ Use partition pruning for large queries

---

### Polars vs Pandas

**RULE:** Prefer Polars for all new data processing code.

**Use Polars for:**
- CSV reading (10x faster than Pandas)
- Schema enforcement
- Large dataset operations
- Transformations

**Use Pandas for:**
- DuckDB result conversion (`.fetchdf()`)
- Legacy compatibility only

**DO NOT:**
- ‚ùå Use Pandas for CSV reading
- ‚ùå Use Pandas for transformations
- ‚ùå Mix Polars and Pandas unnecessarily

**DO:**
- ‚úÖ Use Polars as primary DataFrame library
- ‚úÖ Convert to Pandas only for DuckDB results
- ‚úÖ Benchmark before changing

---

## üîß MODIFICATION CHECKLIST

### Before Modifying Any Script

- [ ] Read entire script to understand context
- [ ] Check what calls this script (grep for filename)
- [ ] Review error handling patterns
- [ ] Understand dependencies (input/output files)
- [ ] Check if changes affect downstream processes

### After Modifying Any Script

- [ ] Test manually with sample data
- [ ] Verify logs show expected output
- [ ] Check downstream dependencies still work
- [ ] Run validation scripts (`Scripts/others/check_*.py`)
- [ ] Test with launchctl if modifying orchestration
- [ ] Update README.md if behavior changes

### When Adding New Transaction Type

- [ ] Add to `file_types` dict in `03_process_daily.py`
- [ ] Add schema to `schemas` dict
- [ ] Add SQL case in `02_fetch_remote_nova_data.sh`
- [ ] Update `sql/build_subscription_view.sql`
- [ ] Update README transaction types section
- [ ] Test with real data end-to-end

---

## üìã QUICK REFERENCE

### Critical Files & Line Numbers

| File | Purpose | Critical Sections |
|------|---------|-------------------|
| `sql/build_subscription_view.sql` | 241-line DuckDB aggregation | Lines 69-95 (missing ACT), Lines 50-108 (upgrades) |
| `Scripts/01_aggregate_user_base.py` | User base aggregation | Lines 27-53 (category mapping) |
| `Scripts/03_process_daily.py` | CSV‚ÜíParquet processor | Schema definitions, Hive partitioning logic |
| `Scripts/04_build_subscription_view.py` | Subscription aggregator | SQL template replacement |

### Remote Server Details

```
Host: 10.26.82.53
User: omadmin
Database: postgres
Table: telefonicaes_sub_mgr_fact
SSH Key: ~/.ssh/id_ed25519
Python: /opt/anaconda3/bin/python
Project: /Users/josemanco/CVAS/CVAS_BEYOND_DATA
```

#### Testing & Validation Scripts (Scripts/others/)
| Script | Purpose |
|--------|---------|
| `check_transactions_parquet_data.py` | Validates transaction Parquet integrity, schema, and partitioning |
| `check_aggregated_parquet_data.py` | Validates aggregated subscription Parquet data |
| `check_users.py` | Validates user data quality and queries by subscription_id/tmuserid/msisdn |
| `extract_music_subscriptions.py` | Extracts music-specific subscriptions for analysis |
| `calculate_lt_ltv.py` | Calculates lifetime and lifetime value metrics |
| `query_msisdn_from_tx.py` | **NEW:** Queries full subscription lifecycle by MSISDN (with MSISDN‚ÜîTMUSERID mapping) |
| `query_tmuserid_from_tx.py` | **NEW:** Queries full subscription lifecycle by TMUSERID (with TMUSERID‚ÜîMSISDN mapping) |

### Testing Commands

```bash
# Test individual scripts
bash 1.GET_NBS_BASE.sh
bash 2.FETCH_DAILY_DATA.sh 2025-01-15
bash 3.PROCESS_DAILY_AND_BUILD_VIEW.sh 2025-01-15

# Test launchd execution
launchctl start com.josemanco.nbs_base
tail -f Logs/1.GET_NBS_BASE.log

# Validate data
python Scripts/others/check_transactions_parquet_data.py
python Scripts/others/check_aggregated_parquet_data.py

# Query transaction data by MSISDN or TMUSERID
python Scripts/others/query_msisdn_from_tx.py 34686516147
python Scripts/others/query_tmuserid_from_tx.py 12345678

# Check Parquet schema
python3 -c "import pyarrow.parquet as pq; print(pq.read_schema('Parquet_Data/aggregated/subscriptions.parquet'))"
```

### Query Scripts

**`Scripts/others/query_msisdn_from_tx.py`**
- Queries transaction data by MSISDN (automatically adds '34' country code if missing)
- Shows MSISDN ‚Üí TMUSERID(s) mapping
- Displays full subscription lifecycle grouped by `subscription_id`:
  - ACT, RENO, DCT, CNR, RFND transactions (sorted by trans_date, trans_type_id)
  - Summary: counts per transaction type, total revenue, total refunded
- Separately shows PPD (Pay Per Download) one-time purchases
- Usage: `python Scripts/others/query_msisdn_from_tx.py <msisdn>`

**`Scripts/others/query_tmuserid_from_tx.py`**
- Queries transaction data by TMUSERID
- Shows TMUSERID ‚Üí MSISDN(s) mapping
- Displays full subscription lifecycle grouped by `subscription_id`:
  - ACT, RENO, DCT, CNR, RFND transactions (sorted by trans_date, trans_type_id)
  - Summary: counts per transaction type, total revenue, total refunded
- Separately shows PPD (Pay Per Download) one-time purchases
- Usage: `python Scripts/others/query_tmuserid_from_tx.py <tmuserid>`

**Query Logic:**
1. Step 1: Find all `subscription_id`s associated with the identifier (from ACT, RENO, DCT)
2. Step 2: Retrieve all transactions (ACT, RENO, DCT, CNR, RFND) for those subscription_ids
3. Step 3: Query PPD transactions directly by the original identifier
4. Note: CNR and RFND don't have `trans_type_id` in source schema; assigned 99 and 100 for sorting

---

## üìä PROJECT VALIDATION SUMMARY (2025-01-27)

### ‚úÖ Validated Components

#### 1. Pipeline Scripts (Core)
| Script | Status | Validation |
|--------|--------|------------|
| `1.GET_NBS_BASE.sh` | ‚úÖ PASS | Log rotation ‚úì, Absolute Python path ‚úì, Error handling ‚úì, Cross-platform date ‚úì |
| `2.FETCH_DAILY_DATA.sh` | ‚úÖ PASS | Log rotation ‚úì, Sequential execution ‚úì, Cross-platform date ‚úì |
| `3.PROCESS_DAILY_AND_BUILD_VIEW.sh` | ‚úÖ PASS | Log rotation ‚úì, Absolute Python path ‚úì, Error handling ‚úì, Cross-platform date ‚úì |
| `Scripts/01_aggregate_user_base.py` | ‚úÖ PASS | Category mapping ‚úì, Service exclusions ‚úì, No PII logging ‚úì |
| `Scripts/02_fetch_remote_nova_data.sh` | ‚úÖ PASS | Cross-platform date ‚úì, SSH connection ‚úì |
| `Scripts/03_process_daily.py` | ‚úÖ PASS | All 6 transaction types ‚úì, Hive partitioning ‚úì, Schema enforcement ‚úì |
| `Scripts/04_build_subscription_view.py` | ‚úÖ PASS | DuckDB aggregation ‚úì, SQL template ‚úì |

#### 2. SQL Queries
| File | Status | Validation |
|------|--------|------------|
| `sql/build_subscription_view.sql` | ‚úÖ PASS | 241 lines ‚úì, All 6 types ‚úì, Hive partitioning ‚úì, Edge cases handled ‚úì |

#### 3. Utility Scripts
| Script | Status | Validation |
|--------|--------|------------|
| `Scripts/utils/log_rotation.sh` | ‚úÖ PASS | 15-day retention ‚úì, Cross-platform date ‚úì |

#### 4. Query & Validation Scripts (Scripts/others/)
| Script | Status | Validation |
|--------|--------|------------|
| `check_transactions_parquet_data.py` | ‚úÖ PASS | Hive partitioning ‚úì, Schema validation ‚úì |
| `check_aggregated_parquet_data.py` | ‚úÖ PASS | Aggregation validation ‚úì |
| `check_users.py` | ‚úÖ PASS | PII display allowed (manual query) ‚úì |
| `extract_music_subscriptions.py` | ‚úÖ PASS | PII display allowed (manual query) ‚úì |
| `calculate_lt_ltv.py` | ‚úÖ PASS | Metrics calculation ‚úì |
| `query_msisdn_from_tx.py` | ‚úÖ PASS | MSISDN‚ÜîTMUSERID mapping ‚úì, Hive partitioning ‚úì, Country code handling ‚úì |
| `query_tmuserid_from_tx.py` | ‚úÖ PASS | TMUSERID‚ÜîMSISDN mapping ‚úì, Hive partitioning ‚úì |

#### 5. Configuration Files
| File | Status | Validation |
|------|--------|------------|
| `requirements.txt` | ‚úÖ PASS | Pinned versions ‚úì (polars==1.34.0, pyarrow==19.0.0, duckdb==1.2.1, pandas==2.2.3) |
| `.gitignore` | ‚úÖ PASS | Data directories excluded ‚úì, Logs excluded ‚úì |

### üîç Key Findings

#### Transaction Type Coverage
- ‚úÖ All 6 transaction types (ACT, RENO, DCT, CNR, RFND, PPD) consistently referenced across:
  - `Scripts/03_process_daily.py` (schema definitions)
  - `Scripts/00_convert_historical.py` (historical conversion)
  - `sql/build_subscription_view.sql` (DuckDB aggregation)
  - Query scripts (lifecycle tracking)

#### Hive Partitioning
- ‚úÖ Implemented in all transaction Parquet writes
- ‚úÖ Used in all DuckDB read operations (`hive_partitioning=true`)
- ‚úÖ Partition format: `year_month=YYYY-MM`
- ‚úÖ Validated in check scripts

#### Python Path Usage
- ‚úÖ All shell scripts use absolute path: `/opt/anaconda3/bin/python`
- ‚úÖ Python scripts use standard shebang: `#!/usr/bin/env python3`
- ‚úÖ Launchd-compatible

#### Cross-Platform Date Handling
- ‚úÖ All shell scripts support both macOS (`date -v`) and Linux (`date -d`)
- ‚úÖ Consistent pattern across all orchestration scripts

#### Log Rotation
- ‚úÖ All 3 orchestration scripts call `rotate_log()` at start
- ‚úÖ 15-day retention enforced
- ‚úÖ Cross-platform compatible

#### Error Handling
- ‚úÖ All orchestration scripts exit with non-zero code on failure
- ‚úÖ Timestamped error logging
- ‚úÖ Validation checks before proceeding

#### PII Protection
- ‚úÖ Pipeline scripts do NOT log PII
- ‚úÖ Query/debugging scripts display PII only when explicitly requested (allowed exception)
- ‚úÖ Clear distinction between automated pipeline and manual debugging tools

#### Schema Enforcement
- ‚úÖ Strict schemas defined in Polars for all 6 transaction types
- ‚úÖ Consistent column names and types
- ‚úÖ Schema validation in check scripts

### üìù Recent Changes (2025-01-27)

1. **New Query Scripts:**
   - Added `Scripts/others/query_msisdn_from_tx.py` - Query subscription lifecycle by MSISDN
   - Added `Scripts/others/query_tmuserid_from_tx.py` - Query subscription lifecycle by TMUSERID
   - Both scripts show identifier mapping (MSISDN‚ÜîTMUSERID)
   - Display full subscription lifecycle grouped by `subscription_id`
   - Separate PPD (Pay Per Download) transactions
   - Automatic country code handling for MSISDN (adds '34' if missing)

2. **Documentation Updates:**
   - Updated `.abacus/rules.md` with query scripts documentation
   - Clarified PII protection exceptions for manual query/debugging scripts
   - Added comprehensive validation summary

### üéØ Compliance Status

| Rule Category | Status | Notes |
|---------------|--------|-------|
| Sequential Pipeline | ‚úÖ COMPLIANT | 1‚Üí2‚Üí3 order enforced |
| Six Transaction Types | ‚úÖ COMPLIANT | All 6 types consistently processed |
| Directory Structure | ‚úÖ COMPLIANT | Immutable structure maintained |
| Schema Enforcement | ‚úÖ COMPLIANT | Strict schemas in all processors |
| Hive Partitioning | ‚úÖ COMPLIANT | All transaction Parquet files partitioned |
| Absolute Python Path | ‚úÖ COMPLIANT | All shell scripts use `/opt/anaconda3/bin/python` |
| Path Management | ‚úÖ COMPLIANT | Relative paths from project root |
| Cross-Platform Date | ‚úÖ COMPLIANT | macOS and Linux support |
| Log Rotation | ‚úÖ COMPLIANT | 15-day retention, all scripts |
| Error Handling | ‚úÖ COMPLIANT | Non-zero exit codes, timestamped logs |
| SQL Query Management | ‚úÖ COMPLIANT | Complex SQL in `sql/` directory |
| Python Dependencies | ‚úÖ COMPLIANT | Exact versions pinned |
| Category Mapping | ‚úÖ COMPLIANT | Business logic preserved |
| NBS_BASE Immutability | ‚úÖ COMPLIANT | Historical files untouched |
| PII Protection | ‚úÖ COMPLIANT | No PII in pipeline logs, allowed in manual query scripts |
| Git Ignore | ‚úÖ COMPLIANT | Data directories excluded |

---

## üéØ TL;DR - MOST IMPORTANT RULES

1. **Sequential Execution:** Never break 1‚Üí2‚Üí3 script order
2. **Six Transaction Types:** Always process all 6 (ACT, RENO, DCT, CNR, RFND, PPD)
3. **Strict Schemas:** Schema changes break everything. Enforce in Polars.
4. **Hive Partitioning:** Required for DuckDB performance. Never remove.
5. **Absolute Python Path:** Use `/opt/anaconda3/bin/python` in shell scripts
6. **No PII in Pipeline Logs:** Never log `tmuserid` or `msisdn` in automated processes (allowed in manual query scripts)
7. **15-Day Log Retention:** Always call `rotate_log()` at start
8. **Git Ignore Data:** Never commit `Daily_Data/`, `Parquet_Data/`, `Logs/`
9. **Edge Cases:** Handle missing ACT records and CPC upgrades
10. **Cross-Platform:** Support both macOS and Linux date commands

---

**Last Updated:** 2025-01-27
**For General Documentation:** See `README.md`
